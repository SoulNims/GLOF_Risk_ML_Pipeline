import pandas as pd
import numpy as np



pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
uncleaned_ml_combined=pd.read_csv("../CSVs/ml_combined_updated2023.csv",index_col=False)


uncleaned_ml_combined.head()



uncleaned_ml_combined.info()


# Split into positives (GLOF = 1) and negatives (GLOF = 0)
uncleaned_ml_pos = uncleaned_ml_combined[uncleaned_ml_combined["GLOF"] == 1].reset_index(drop=True)
uncleaned_ml_neg = uncleaned_ml_combined[uncleaned_ml_combined["GLOF"] == 0].reset_index(drop=True)

# Save both to CSV
uncleaned_ml_pos.to_csv("../CSVs/ml_pos_updated2023.csv", index=False)
uncleaned_ml_neg.to_csv("../CSVs/ml_neg_updated2023.csv", index=False)


uncleaned_ml_pos.info()
uncleaned_ml_pos


uncleaned_ml_neg.info()


uncleaned_ml_pos.isnull().mean()*100


uncleaned_ml_neg.isnull().mean()*100


uncleaned_ml_combined.isnull().mean()*100





# Load Data
df_comb = pd.read_csv("../CSVs/ml_combined_updated2023.csv")
df_pos  = pd.read_csv("../CSVs/ml_pos_updated2023.csv")

# Complete Case Analysis (CCA)
df_comb_dropna = df_comb.dropna()
df_pos_dropna  = df_pos.dropna()

print(df_comb.shape)
print(df_pos.shape)
print(df_comb_dropna.shape)
print(df_pos_dropna.shape)






df_pos.info


df_comb.head()


# Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import resample
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.base import clone



# Load Data
df_comb = pd.read_csv("../CSVs/ml_combined_updated2023.csv")

# Complete Case Analysis (CCA)
df_comb = df_comb.dropna()

# Features and Target
X = df_comb.drop("GLOF", axis=1)
y = df_comb["GLOF"]

# Preprocessing
# Only one-hot encode Lake_type_simplified,
# all others are numeric (including 0/1 binary flags)
cat_features = ["Lake_type_simplified"]
exclude_cols = cat_features + ["Latitude", "Longitude","Year_final"]
num_features = [col for col in X.columns if col not in exclude_cols]

preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features)
    ]
)
# check class balance
print(df_comb["GLOF"].value_counts())

df_temp = df_comb.dropna()
count_pos = df_temp[df_temp["GLOF"] == 1].shape[0]
print("Rows with GLOF = 1 after dropna():", count_pos)


# Ensemble Creation Function
# Undersample majority class into many subsets, train separate classifiers

def create_ensemble(X_train, y_train, n_models=5, base_model=None):
    """
    Create an ensemble of classifiers with undersampling.
    
    Parameters:
    - X_train, y_train: training data
    - n_models: number of models in ensemble
    - base_model: the classifier to use (e.g., RandomForestClassifier(), LogisticRegression(), XGBClassifier())
    """
    if base_model is None:
        base_model = RandomForestClassifier()  # default

    models = []

    # Split into minority (1) and majority (0)
    X_minority = X_train[y_train == 1]
    y_minority = y_train[y_train == 1]
    X_majority = X_train[y_train == 0]
    y_majority = y_train[y_train == 0]

    for i in range(n_models):
        # Undersample majority class
        X_majority_resampled, y_majority_resampled = resample(
            X_majority, y_majority,
            replace=False,
            n_samples=len(y_minority),
            random_state=i
        )

        # Combine resampled majority + minority
        X_balanced = pd.concat([X_majority_resampled, X_minority])
        y_balanced = pd.concat([y_majority_resampled, y_minority])

        # Clone base_model so each one is fresh
        clf = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("classifier", clone(base_model).set_params(random_state=i))
            if hasattr(base_model, "random_state") else
            ("classifier", clone(base_model))
        ])

        # Train
        clf.fit(X_balanced, y_balanced)
        models.append(clf)

    return models

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

# Helper: Ensemble Prediction
def ensemble_predict(models, X):
    preds = np.array([m.predict(X) for m in models])
    # Majority vote: if at least half the models predict 1, label as 1 (else 0)
    final_preds = (np.sum(preds, axis=0) >= (len(models) / 2)).astype(int)
    return final_preds



import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
def evaluate_ensemble(name, ensemble_models, X_test, y_test, plot=True):
    # Predictions
    y_pred = ensemble_predict(ensemble_models, X_test)

    # Classification report
    print(f"\n=== {name} Ensemble: Classification Report ===")
    print(classification_report(y_test, y_pred))

    # Confusion matrix
    print(f"\n=== {name} Ensemble: Confusion Matrix ===")
    print(confusion_matrix(y_test, y_pred))

    # ROC-AUC (use predicted probabilities)
    y_probas = np.mean([m.predict_proba(X_test)[:, 1] for m in ensemble_models], axis=0)
    auc_score = roc_auc_score(y_test, y_probas)
    print(f"\n{name} Ensemble: ROC-AUC = {auc_score:.4f}")

    # Plot ROC curve
    if plot:
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.figure(figsize=(7,5))
        plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color="blue")
        plt.plot([0, 1], [0, 1], "k--", label="Chance")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve - {name} Ensemble")
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()



import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_curve,
    roc_auc_score
)

def evaluate_ensemble(name, ensemble_models, X_test, y_test, plot=True):
    # --- Predictions ---
    y_pred = ensemble_predict(ensemble_models, X_test)

    # --- Classification Report ---
    print(f"\n=== {name} Ensemble: Classification Report ===")
    print(classification_report(y_test, y_pred))

    # --- Confusion Matrix ---
    print(f"\n=== {name} Ensemble: Confusion Matrix ===")
    print(confusion_matrix(y_test, y_pred))

    # --- ROC-AUC (use predicted probabilities) ---
    y_probas = np.mean([m.predict_proba(X_test)[:, 1] for m in ensemble_models], axis=0)
    auc_score = roc_auc_score(y_test, y_probas)
    print(f"\n{name} Ensemble: ROC-AUC = {auc_score:.4f}")

    # --- Feature Importances (for tree-based models only) ---
    feature_names = ensemble_models[0].named_steps["preprocessor"].get_feature_names_out()
    importances = []

    for m in ensemble_models:
        clf = m.named_steps["classifier"]
        if hasattr(clf, "feature_importances_"):
            importances.append(clf.feature_importances_)

    if importances:
        mean_importances = np.mean(importances, axis=0)
        sorted_idx = np.argsort(mean_importances)[::-1]

        print(f"\n=== {name} Ensemble: Mean Feature Importances ===")
        for idx in sorted_idx[:15]:  # top 15 features
            print(f"{feature_names[idx]:40s}  {mean_importances[idx]:.4f}")

        # --- Plot top features ---
        plt.figure(figsize=(8, 6))
        plt.barh(np.array(feature_names)[sorted_idx[:15]][::-1],
                 mean_importances[sorted_idx[:15]][::-1],
                 color="teal")
        plt.title(f"Top Feature Importances - {name} Ensemble")
        plt.xlabel("Mean Importance")
        plt.tight_layout()
        plt.show()

    # --- ROC Curve ---
    if plot:
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.figure(figsize=(7,5))
        plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}", color="blue")
        plt.plot([0, 1], [0, 1], "k--", label="Chance")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve - {name} Ensemble")
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()



# ----------------------------
# Train-Test Split
# ----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ----------------------------
# Random Forest Ensemble
# ----------------------------
rf_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=RandomForestClassifier()
)
evaluate_ensemble("Random Forest", rf_models, X_test, y_test)


# ----------------------------
# Logistic Regression Ensemble
# ----------------------------
lr_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=LogisticRegression(max_iter=1000)
)
evaluate_ensemble("Logistic Regression", lr_models, X_test, y_test)


# ----------------------------
# XGBoost Ensemble
# ----------------------------
xgb_models = create_ensemble(
    X_train, y_train, n_models=5,
    base_model=XGBClassifier(
        eval_metric="logloss"
    )
)
evaluate_ensemble("XGBoost", xgb_models, X_test, y_test)


print(df_comb["GLOF"].value_counts())






import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/ml_combined_updated2023.csv")

# Define target column
target = "GLOF"   # change if yours is "Activity" or another name

# Drop unwanted + target
exclude_cols = ["Year_final", "Latitude", "Longitude","Elevation_m", target]
X = df.drop(columns=exclude_cols)
y = df[target]

# Separate categorical & numeric columns
categorical_cols = ["Lake_type_simplified"]
numeric_cols = [col for col in X.columns if col not in categorical_cols]

# Keep only complete rows (CCA subset)
X_cca = X.dropna()

print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%)
# ==============================================
X_masked = X_cca.copy()
rng = np.random.default_rng(42)

# Mask numeric columns
for col in numeric_cols:
    mask = rng.uniform(size=X_cca.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# Mask categorical column
for col in categorical_cols:
    mask = rng.uniform(size=X_cca.shape[0]) < 0.2
    X_masked.loc[mask, col] = np.nan

# ==============================================
# 3. Define preprocessors
# ==============================================
numeric_transformer = Pipeline(steps=[
    ("imputer", IterativeImputer(random_state=42, max_iter=10))
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols)
    ]
)

# ==============================================
# 4. Fit & Transform (Impute)
# ==============================================
X_imputed = preprocessor.fit_transform(X_masked)

# Rebuild imputed dataframe
imputed_feature_names = (
    numeric_cols +
    list(preprocessor.named_transformers_["cat"]["onehot"].get_feature_names_out(categorical_cols))
)
X_imputed = pd.DataFrame(X_imputed, columns=imputed_feature_names, index=X_masked.index)

# ==============================================
# 5. Evaluate Imputation Accuracy
# ==============================================
print("\n===Iterative Imputer==")
print("\n=== Numeric RMSE ===")
for col in numeric_cols:
    missing_mask = X_masked[col].isna()
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col]
        imputed_vals = X_imputed.loc[missing_mask, col]
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy ===")
for col in categorical_cols:
    missing_mask = X_masked[col].isna()
    if missing_mask.any():
        # Get the imputed column set for this categorical
        onehot_cols = [c for c in X_imputed.columns if c.startswith(col + "_")]
        imputed_cat = X_imputed.loc[missing_mask, onehot_cols].idxmax(axis=1).str.replace(col + "_", "")
        true_cat = X_cca.loc[missing_mask, col].astype(str)
        acc = accuracy_score(true_cat, imputed_cat)
        print(f"{col}: Accuracy = {acc:.3f}")










pip install miceforest






import pandas as pd
import numpy as np
import miceforest as mf
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only
X = df.drop(columns=exclude_cols)

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

# Keep only complete rows (CCA subset) + reset index
X_cca = X.dropna().reset_index(drop=True)
print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%)
# ==============================================
X_masked_manual = X_cca.copy()
rng = np.random.default_rng(42)

for col in X_masked_manual.columns:
    mask = rng.uniform(size=X_masked_manual.shape[0]) < 0.2
    X_masked_manual.loc[mask, col] = np.nan

# ==============================================
# 3. Run miceforest Imputation
# ==============================================
kernel_manual = mf.ImputationKernel(
    X_masked_manual,
    save_all_iterations_data=True,
    random_state=42
)

kernel_manual.mice(iterations=10, n_datasets=5)

X_imputed_manual = kernel_manual.complete_data(
    dataset=0,
    iteration=kernel_manual.iteration_count() - 1
)

# ==============================================
# 4. Evaluate Imputation Accuracy
# ==============================================
print("\n=== Numeric RMSE (Manual) ===")
for col in X_cca.select_dtypes(include=[np.number]).columns:
    missing_mask = X_masked_manual[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].values
        imputed_vals = X_imputed_manual.loc[missing_mask, col].values
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy (Manual) ===")
for col in X_cca.select_dtypes(include=["category"]).columns:
    missing_mask = X_masked_manual[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].astype(str).values
        imputed_vals = X_imputed_manual.loc[missing_mask, col].astype(str).values
        acc = accuracy_score(true_vals, imputed_vals)
        print(f"{col}: Accuracy = {acc:.3f}")








import pandas as pd
import numpy as np
import miceforest as mf
from sklearn.metrics import mean_squared_error, accuracy_score

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/ml_combined_updated2023.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only
X = df.drop(columns=exclude_cols)

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

# Keep only complete rows (CCA subset) + reset index
X_cca = X.dropna().reset_index(drop=True)
print(f"Original shape: {X.shape}, After CCA: {X_cca.shape}")

# ==============================================
# 2. Artificial Missingness (20%) with ampute_data
# ==============================================
X_masked_ampute = mf.ampute_data(
    X_cca,
    perc=0.2,
    random_state=42
)

# ==============================================
# 3. Run miceforest Imputation
# ==============================================
kernel_ampute = mf.ImputationKernel(
    X_masked_ampute,
    save_all_iterations_data=True,
    random_state=42
)
kernel_ampute.mice(iterations=10, n_datasets=5)

X_imputed_ampute = kernel_ampute.complete_data(
    dataset=0,
    iteration=kernel_ampute.iteration_count() - 1
)

# ==============================================
# 4. Evaluate Imputation Accuracy
# ==============================================
print("\n=== Numeric RMSE (Ampute) ===")
for col in X_cca.select_dtypes(include=[np.number]).columns:
    missing_mask = X_masked_ampute[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].values
        imputed_vals = X_imputed_ampute.loc[missing_mask, col].values
        rmse = np.sqrt(mean_squared_error(true_vals, imputed_vals))
        print(f"{col}: RMSE = {rmse:.3f}")

print("\n=== Categorical Accuracy (Ampute) ===")
for col in X_cca.select_dtypes(include=["category"]).columns:
    missing_mask = X_masked_ampute[col].isna().values
    if missing_mask.any():
        true_vals = X_cca.loc[missing_mask, col].astype(str).values
        imputed_vals = X_imputed_ampute.loc[missing_mask, col].astype(str).values
        acc = accuracy_score(true_vals, imputed_vals)
        print(f"{col}: Accuracy = {acc:.3f}")



#install plotnine
!pip install plotnine --quiet


# Plot observed vs imputed distributions: Manual Masking
kernel_manual.plot_imputed_distributions()



# Plot observed vs imputed distributions: Ampute Masking
!pip install plotnine --quiet
kernel_ampute.plot_imputed_distributions()


import seaborn as sns
import matplotlib.pyplot as plt

def compare_kde(col, X_true, X_imp_manual=None, X_imp_ampute=None):
    plt.figure(figsize=(8,5))

    # True distribution (from CCA complete cases)
    sns.kdeplot(X_true[col], label="True (Complete Cases)", color="black", lw=2)

    # Manual masking imputations
    if X_imp_manual is not None:
        sns.kdeplot(X_imp_manual[col], label="Imputed (Manual)", color="red", lw=2)

    # Ampute masking imputations
    if X_imp_ampute is not None:
        sns.kdeplot(X_imp_ampute[col], label="Imputed (Ampute)", color="blue", lw=2)

    plt.title(f"Distribution Comparison: {col}")
    plt.xlabel(col)
    plt.ylabel("Density")
    plt.legend()
    plt.show()




# Compare expansion rate distributions
compare_kde("5y_expansion_rate", X_cca, X_imputed_manual, X_imputed_ampute)
compare_kde("10y_expansion_rate", X_cca, X_imputed_manual, X_imputed_ampute)

# Compare elevation distributions
compare_kde("Elevation_m", X_cca, X_imputed_manual, X_imputed_ampute)



# Basic statistics for each numeric column
desc = X_cca.describe().T  # mean, std, min, max, quartiles
# Add extra columns for range and coefficient of variation
desc["range"] = desc["max"] - desc["min"]
desc["cv"] = desc["std"] / desc["mean"]  # coefficient of variation
print(desc[["mean", "std", "min", "25%", "50%", "75%", "max", "range", "cv"]])



X_imputed


# Add target back
final_df = X_imputed.copy()
final_df[target] = df.loc[X_imputed.index, target].values
final_df







# ==============================================
# Imputation with MICE (miceforest) on full dataset
# With imputation flags for key variables
# ==============================================

import pandas as pd
import numpy as np
import miceforest as mf

# ==============================================
# 1. Load Data
# ==============================================
df = pd.read_csv("../CSVs/ml_combined_updated2023.csv")

# Define target and excluded columns
target = "GLOF"   # or "Activity"
exclude_cols = ["Year_final", target]

# Features only (excluding target + year)
X = df.drop(columns=exclude_cols).copy()

# Ensure categorical dtype
X["Lake_type_simplified"] = X["Lake_type_simplified"].astype("category")

print(f"Original shape: {X.shape}")
print("Missing values (%):")
print(X.isnull().mean() * 100)

# ==============================================
# 2. Run miceforest Imputation (real missingness)
# ==============================================
kernel = mf.ImputationKernel(
    X,
    num_datasets=5,                # <-- Correct parameter
    save_all_iterations_data=True,
    random_state=42
)

# Run MICE: 10 iterations
kernel.mice(iterations=10)

# ==============================================
# 3. Retrieve ALL imputed datasets + add flags
# ==============================================
# Variables to flag (based on your missingness analysis)
flag_vars = [
    "Lake_area_calculated_ha",
    "5y_expansion_rate",
    "10y_expansion_rate",
    "glacier_area_ha",
    "slope_glac_to_lake",
    "nearest_glacier_dist_m",
    "glacier_elev_m",
]

# Store finished datasets in a dictionary
X_imputed_dict = {}
last_iter = kernel.iteration_count() - 1

for d in range(5):   # loop over 0–4
    X_imp = kernel.complete_data(dataset=d, iteration=last_iter).copy()
    
    # Add imputation flags
    for col in flag_vars:
        X_imp[f"is_imputed_{col}"] = X[col].isna().astype(int)
    
    # Reattach target
    X_imp[target] = df[target].values
    
    # Save to dictionary
    X_imputed_dict[d] = X_imp
    print(f"Dataset {d} shape: {X_imp.shape}")

# Example: access dataset 0
X_imputed_0 = X_imputed_dict[0]
print("\nFinal dataset 0 shape:", X_imputed_0.shape)



#Accessing the datasets:
X_imputed_0 = X_imputed_dict[0]
X_imputed_1 = X_imputed_dict[1]
X_imputed_2 = X_imputed_dict[2]
X_imputed_3 = X_imputed_dict[3]
X_imputed_4 = X_imputed_dict[4]






X_imputed_0.head(20)





# ==============================================
# MICE-Imputed Pipeline Setup (WITH imputation flags)
# ==============================================

# Features and Target
X_MICE_with_flags = X_imputed_0.drop("GLOF", axis=1)   # <- df_MICE is your completed dataset from MICE
y_MICE_with_flags = X_imputed_0["GLOF"]

# Define categorical + numerical features
cat_features_with_flags = ["Lake_type_simplified"]
exclude_cols_with_flags = cat_features_with_flags + ["Year_final", "Latitude", "Longitude"]
num_features_with_flags = [col for col in X_MICE_with_flags.columns if col not in exclude_cols_with_flags]

# Preprocessing pipeline
preprocessor_MICE_with_flags = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features_with_flags),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features_with_flags)
    ]
)



from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import numpy as np

# ===================================================
# Helper function to evaluate a model (WITH imputation flags)
# ===================================================
def evaluate_model_with_flags(name, model, X_train, X_test, y_train, y_test, preprocessor):
    # Build pipeline
    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])
    
    # Fit
    pipeline.fit(X_train, y_train)
    
    # Predict
    y_pred = pipeline.predict(X_test)
    y_probas = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None
    
    # Classification Report
    print(f"\n=== {name} (WITH Imputation Flags) ===")
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    # ROC-AUC
    if y_probas is not None:
        auc_score = roc_auc_score(y_test, y_probas)
        print(f"ROC-AUC = {auc_score:.4f}")
        
        # Plot ROC
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc_score:.2f})")

# ===================================================
# Train-test split (WITH imputation flags)
# ===================================================
from sklearn.model_selection import train_test_split
X_train_flags, X_test_flags, y_train_flags, y_test_flags = train_test_split(
    X_MICE_with_flags, y_MICE_with_flags, test_size=0.2, random_state=42, stratify=y_MICE_with_flags
)

# ===================================================
# Try multiple classifiers (WITH imputation flags)
# ===================================================
plt.figure(figsize=(7,5))

evaluate_model_with_flags("Logistic Regression", LogisticRegression(max_iter=1000, class_weight="balanced"),
               X_train_flags, X_test_flags, y_train_flags, y_test_flags, preprocessor_MICE_with_flags)

evaluate_model_with_flags("Random Forest", RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42),
               X_train_flags, X_test_flags, y_train_flags, y_test_flags, preprocessor_MICE_with_flags)

evaluate_model_with_flags("Gradient Boosting", GradientBoostingClassifier(),
               X_train_flags, X_test_flags, y_train_flags, y_test_flags, preprocessor_MICE_with_flags)

evaluate_model_with_flags("SVM (Linear)", SVC(probability=True, kernel="linear", class_weight="balanced", random_state=42),
               X_train_flags, X_test_flags, y_train_flags, y_test_flags, preprocessor_MICE_with_flags)

plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves Across Models - MICE Imputation (WITH Imputation Flags)")
plt.legend()
plt.grid(True)
plt.show()






# ==============================================
# MICE-Imputed Pipeline Setup (WITHOUT imputation flags)
# ==============================================

# Features and Target
X_MICE_no_flags = X_imputed_0.drop("GLOF", axis=1)   # <- df_MICE is your completed dataset from MICE
y_MICE_no_flags = X_imputed_0["GLOF"]

# Define categorical + numerical features
cat_features_no_flags = ["Lake_type_simplified"]
exclude_cols_no_flags = cat_features_no_flags + [
    "Year_final",
    "Latitude",
    "Longitude",
    "is_imputed_Lake_area_calculated_ha",
    "is_imputed_5y_expansion_rate",
    "is_imputed_10y_expansion_rate",
    "is_imputed_glacier_area_ha",
    "is_imputed_slope_glac_to_lake",
    "is_imputed_nearest_glacier_dist_m",
    "is_imputed_glacier_elev_m"
]
num_features_no_flags = [col for col in X_MICE_no_flags.columns if col not in exclude_cols_no_flags]

# Preprocessing pipeline
preprocessor_MICE_no_flags = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_features_no_flags),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_features_no_flags)
    ]
)




# ===================================================
# Helper function to evaluate a model (WITHOUT imputation flags)
# ===================================================
def evaluate_model_without_flags(name, model, X_train, X_test, y_train, y_test, preprocessor):
    # Build pipeline
    pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])
    
    # Fit
    pipeline.fit(X_train, y_train)
    
    # Predict
    y_pred = pipeline.predict(X_test)
    y_probas = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline, "predict_proba") else None
    
    # Classification Report
    print(f"\n=== {name} (WITHOUT Imputation Flags) ===")
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    # ROC-AUC
    if y_probas is not None:
        auc_score = roc_auc_score(y_test, y_probas)
        print(f"ROC-AUC = {auc_score:.4f}")
        
        # Plot ROC
        fpr, tpr, _ = roc_curve(y_test, y_probas)
        plt.plot(fpr, tpr, label=f"{name} (AUC={auc_score:.2f})")


# ===================================================
# Train-test split (WITHOUT imputation flags)
# ===================================================
X_train_no_flags, X_test_no_flags, y_train_no_flags, y_test_no_flags = train_test_split(
    X_MICE_no_flags, y_MICE_no_flags, test_size=0.2, random_state=42, stratify=y_MICE_no_flags
)

# ===================================================
# Try multiple classifiers (WITHOUT imputation flags)
# ===================================================
plt.figure(figsize=(7,5))

evaluate_model_without_flags("Logistic Regression", LogisticRegression(max_iter=1000, class_weight="balanced"),
               X_train_no_flags, X_test_no_flags, y_train_no_flags, y_test_no_flags, preprocessor_MICE_no_flags)

evaluate_model_without_flags("Random Forest", RandomForestClassifier(n_estimators=200, class_weight="balanced", random_state=42),
               X_train_no_flags, X_test_no_flags, y_train_no_flags, y_test_no_flags, preprocessor_MICE_no_flags)

evaluate_model_without_flags("Gradient Boosting", GradientBoostingClassifier(),
               X_train_no_flags, X_test_no_flags, y_train_no_flags, y_test_no_flags, preprocessor_MICE_no_flags)

evaluate_model_without_flags("SVM (Linear)", SVC(probability=True, kernel="linear", class_weight="balanced", random_state=42),
               X_train_no_flags, X_test_no_flags, y_train_no_flags, y_test_no_flags, preprocessor_MICE_no_flags)

plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves Across Models - MICE Imputation (WITHOUT Imputation Flags)")
plt.legend()
plt.grid(True)
plt.show()






# Cross-validation comparison across models
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

models = {
    "LogReg": LogisticRegression(max_iter=1000, class_weight="balanced"),
    "RF": RandomForestClassifier(n_estimators=300, class_weight="balanced", random_state=42),
    "GB": GradientBoostingClassifier(random_state=42),
    "SVMlin": SVC(probability=True, kernel="linear", class_weight="balanced", random_state=42),
}

def cv_eval(name, model):
    pipe = Pipeline([("pre", preprocessor_MICE_no_flags), ("clf", model)])
    scores = cross_validate(
        pipe,
        X_MICE_no_flags, y_MICE_no_flags,
        cv=cv,
        scoring={"recall":"recall", "roc_auc":"roc_auc", "f1":"f1"},
        n_jobs=-1, return_train_score=False
    )
    print(f"{name}: recall={scores['test_recall'].mean():.3f}±{scores['test_recall'].std():.3f} | "
          f"AUC={scores['test_roc_auc'].mean():.3f}±{scores['test_roc_auc'].std():.3f} | "
          f"F1={scores['test_f1'].mean():.3f}")

for n,m in models.items():
    cv_eval(n, m)



# Hyperparameter tuning for Gradient Boosting
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier

gb = Pipeline([("pre", preprocessor_MICE_no_flags),
               ("clf", GradientBoostingClassifier(random_state=42))])

param_grid = {
    "clf__n_estimators": [150, 200, 300],
    "clf__learning_rate": [0.05, 0.1, 0.2],
    "clf__max_depth": [2, 3, 4],
    "clf__subsample": [0.8, 1.0],
    "clf__min_samples_leaf": [1, 2]
}

grid = GridSearchCV(
    gb, param_grid,
    scoring="recall",   # prioritize catching hazardous lakes
    cv=cv, n_jobs=-1
)
grid.fit(X_MICE_no_flags, y_MICE_no_flags)
print("Best params:", grid.best_params_)
print("CV recall:", grid.best_score_)



# ==============================================
# Hyperparameter tuning for Logistic Regression (NO imputation flags)
# ==============================================
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

pipe_lg = Pipeline([
    ("pre", preprocessor_MICE_no_flags),
    ("clf", LogisticRegression(max_iter=2000, class_weight="balanced", random_state=42))
])

param_grid = {
    "clf__C": [0.01, 0.1, 1, 5, 10],
    "clf__penalty": ["l2"],   # 'l1' works only with liblinear/saga
    "clf__solver": ["liblinear", "lbfgs", "saga"]
}

grid_lg = GridSearchCV(
    pipe_lg,
    param_grid,
    scoring="recall",   # prioritize detecting hazardous lakes
    cv=5,
    n_jobs=-1
)

grid_lg.fit(X_MICE_no_flags, y_MICE_no_flags)

print("Best Parameters:", grid_lg.best_params_)
print(f"Best Mean Recall (CV): {grid_lg.best_score_:.3f}")



evaluate_model_without_flags(
    "Tuned Logistic Regression",
    grid_lg.best_estimator_.named_steps["clf"],
    X_train_no_flags, X_test_no_flags,
    y_train_no_flags, y_test_no_flags,
    preprocessor_MICE_no_flags
)



# ==============================================
# Hyperparameter tuning for Gradient Boosting (NO imputation flags)
# ==============================================
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

pipe_gb = Pipeline([
    ("pre", preprocessor_MICE_no_flags),
    ("clf", GradientBoostingClassifier(random_state=42))
])

param_grid = {
    "clf__n_estimators": [100, 200, 300],
    "clf__learning_rate": [0.05, 0.1, 0.2],
    "clf__max_depth": [2, 3, 4],
    "clf__subsample": [0.8, 1.0],
    "clf__min_samples_leaf": [1, 2]
}

grid_gb = GridSearchCV(
    pipe_gb,
    param_grid,
    scoring="recall",   # prioritize hazardous-lake detection
    cv=5,
    n_jobs=-1
)

grid_gb.fit(X_MICE_no_flags, y_MICE_no_flags)

print("Best Parameters:", grid_gb.best_params_)
print(f"Best Mean Recall (CV): {grid_gb.best_score_:.3f}")



evaluate_model_without_flags(
    "Tuned Gradient Boosting",
    grid_gb.best_estimator_.named_steps["clf"],
    X_train_no_flags, X_test_no_flags,
    y_train_no_flags, y_test_no_flags,
    preprocessor_MICE_no_flags
)



# ===================================================
# Hyperparameter tuning for Random Forest (NO imputation flags)
# ===================================================
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

# --- Build pipeline ---
pipe_rf = Pipeline([
    ("pre", preprocessor_MICE_no_flags),
    ("clf", RandomForestClassifier(random_state=42))
])

# --- Parameter grid ---
param_grid_rf = {
    "clf__n_estimators": [100, 200, 300],
    "clf__max_depth": [5, 10, 15, None],
    "clf__min_samples_split": [2, 5, 10],
    "clf__min_samples_leaf": [1, 2, 4],
    "clf__max_features": ["sqrt", "log2"],
    "clf__bootstrap": [True, False],
}

grid_rf = GridSearchCV(
    pipe_rf,
    param_grid_rf,
    scoring="recall",
    cv=5,
    n_jobs=-1,
    verbose=0   # set to 0 for no output, or 1 for minimal progress
)


# --- Fit on data ---
grid_rf.fit(X_MICE_no_flags, y_MICE_no_flags)

# --- Results ---
print("Best Parameters:", grid_rf.best_params_)
print(f"Best Mean Recall (CV): {grid_rf.best_score_:.3f}")



# --- Evaluate best model ---
evaluate_model_without_flags(
    "Tuned Random Forest",
    grid_rf.best_estimator_.named_steps["clf"],
    X_train_no_flags, X_test_no_flags,
    y_train_no_flags, y_test_no_flags,
    preprocessor_MICE_no_flags,
)





# ===================================================
# Retrain final Logistic Regression model on full dataset
# ===================================================
final_logreg = grid_lg.best_estimator_

# Fit model on the entire dataset (training + test combined)
final_logreg.fit(X_MICE_no_flags, y_MICE_no_flags)

print("✅ Final Logistic Regression model retrained on full dataset.")



# ===================================================
# Generate final hazard probabilities for all lakes
# ===================================================
hazard_df = X_imputed_dict[0].copy()   # use the first fully imputed dataset
X_final = hazard_df.drop(columns=["GLOF"])

# Predict hazard probabilities
hazard_df["Hazard_Prob"] = final_logreg.predict_proba(X_final)[:, 1]

print("✅ Added Hazard_Prob column to imputed dataset.")
hazard_df.head()



# ===================================================
# Interactive GLOF Hazard Map - Continuous Colors + Linear Size Scaling
# ===================================================
import folium
from branca.colormap import LinearColormap

# ===================================================
# 1. Initialize OpenStreetMap base (token-free)
# ===================================================
hazard_map = folium.Map(
    location=[28.2, 87.0],   # Center near Makalu-Barun / eastern Nepal
    zoom_start=7,
    tiles="Cartodb Positron",
    attr="© OpenStreetMap contributors"
)

# ===================================================
# 2. Create smooth continuous color gradient
# ===================================================
colormap = LinearColormap(
    ['green', 'yellow', 'orange', 'red', 'darkred'],
    vmin=0, vmax=1,
    caption='Hazard Probability'
)
colormap.add_to(hazard_map)

# ===================================================
# 3. Plot each lake as a circle marker
# ===================================================
for _, row in hazard_df.iterrows():
    prob = row.Hazard_Prob
    area = row[area_col]
    color = colormap(prob)

    # Linear circle size scaling
    radius = max(2, min(area / 30, 10))

    # Popup with all lake information
    popup_html = f"""
    <b>Lake Information</b><br>
    <b>Latitude:</b> {row.Latitude:.4f}<br>
    <b>Longitude:</b> {row.Longitude:.4f}<br>
    <b>Lake Area (ha):</b> {area:.2f}<br>
    <b>Elevation (m):</b> {row.Elevation_m:.0f}<br>
    <b>Lake Type:</b> {row.Lake_type_simplified}<br>
    <b>Supraglacial:</b> {row.is_supraglacial}<br>
    <b>Glacier Area (ha):</b> {row.glacier_area_ha:.2f}<br>
    <b>Slope glac→lake (°):</b> {row.slope_glac_to_lake:.2f}<br>
    <b>Glacier Contact:</b> {row.glacier_contact}<br>
    <b>Glacier Touch Count:</b> {row.glacier_touch_count}<br>
    <b>Nearest Glacier Dist (m):</b> {row.nearest_glacier_dist_m:.0f}<br>
    <b>Glacier Elev (m):</b> {row.glacier_elev_m:.0f}<br>
    <b>5-yr Expansion Rate:</b> {row['5y_expansion_rate']:.3f}<br>
    <b>10-yr Expansion Rate:</b> {row['10y_expansion_rate']:.3f}<br>
    <b>Observed GLOF:</b> {row.GLOF}<br>
    <b><font color='{color}'>Hazard Probability:</font></b> {prob:.2f}
    """

    folium.CircleMarker(
        location=[row.Latitude, row.Longitude],
        radius=radius,
        color=None,
        fill=True,
        fill_color=color,
        fill_opacity=0.85,
        popup=folium.Popup(popup_html, max_width=350)
    ).add_to(hazard_map)

# ===================================================
# 4. Add layer control and show map
# ===================================================
folium.LayerControl().add_to(hazard_map)
print("✅ Interactive hazard map generated (continuous color + smaller linear circle scaling).")

hazard_map









##  Save final tuned model for deployment
import pickle

with open("../model.pkl", "wb") as f:
    pickle.dump(final_logreg, f)




hazard_df


hazard_df.to_csv("../CSVs/hazard_df.csv", index=False)



# ===================================================
# Export final imputed dataset with hazard probabilities
# ===================================================

# 1️⃣ Select the first imputed dataset (includes GLOF label)
hazard_probabilities = X_imputed_dict[0].copy()

# 2️⃣ Separate features and generate hazard probabilities
X_final = hazard_probabilities.drop(columns=["GLOF"])
hazard_probabilities["Hazard_Prob"] = final_logreg.predict_proba(X_final)[:, 1]

# 3️⃣ Export to the CSVs folder (no index column)
hazard_probabilities.to_csv("../CSVs/hazard_probabilities.csv", index=False)

print("✅ Exported final dataset with hazard probabilities → ../CSVs/hazard_probabilities.csv")
print("Shape:", hazard_probabilities.shape)
print("Columns:", hazard_probabilities.columns.tolist())
print("Hazard_Prob range:", (hazard_probabilities['Hazard_Prob'].min(), hazard_probabilities['Hazard_Prob'].max()))



hazard_probabilities


hazard_probabilities.info



