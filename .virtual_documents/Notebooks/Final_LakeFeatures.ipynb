#GEE Authentication and Initialization


!pip install earthengine-api geemap --quiet



import ee
import geemap


ee.Authenticate()


ee.Initialize(project='fleet-hawk-463015-h5')



import pandas as pd
df_pre_enrich=pd.read_csv("../CSVs/df_final_with_laketypesorted_pre.csv")
df_pre_enrich.head(10)


df_subset = df_pre_enrich.iloc[0:50]   # rows 0–49 (i.e., 1–50 if you’re counting from 1)



df_subset.head(5)





import ee

# 100% server-side function for nonglof lakes
def detect_lake_from_point(point, year, search_radius=3000, ndwi_thresh=0.3):
    """
    Detect lake polygons around a point for a given year using Landsat NDWI.
    
    Args:
        point: ee.Geometry.Point
        year: ee.Number or int (Year_final)
        search_radius: buffer radius around point (m)
        ndwi_thresh: NDWI threshold for water detection
    
    Returns:
        ee.Feature with geometry = largest detected lake polygon (or the point if none)
        property: 'Lake_area_calculated_ha'
    """
    point = ee.Geometry(point)
    year  = ee.Number(year).toInt()
    aoi   = point.buffer(search_radius)

    # --- Cloud mask ---
    def mask_landsat_clouds(img):
        qa = img.select('QA_PIXEL')
        cloud        = 1 << 3
        cloud_shadow = 1 << 4
        snow         = 1 << 5
        mask = (qa.bitwiseAnd(cloud).eq(0)
                  .And(qa.bitwiseAnd(cloud_shadow).eq(0))
                  .And(qa.bitwiseAnd(snow).eq(0)))
        return img.updateMask(mask)

    # --- Scale reflectance ---
    def scale_sr(img):
        optical = img.select('SR_B[0-9]*').multiply(0.0000275).add(-0.2)
        return img.addBands(optical, overwrite=True)

    # --- Add NDWI ---
    def add_ndwi(img, use_l8_like):
        green = ee.Image(ee.Algorithms.If(use_l8_like, img.select('SR_B3'), img.select('SR_B2')))
        nir   = ee.Image(ee.Algorithms.If(use_l8_like, img.select('SR_B5'), img.select('SR_B4')))
        ndwi  = green.subtract(nir).divide(green.add(nir)).rename('NDWI')
        return img.addBands(ndwi)

    # --- Get fall Landsat collection ---
    def get_fall_collection(y_num):
        y_num      = ee.Number(y_num).toInt()
        use_l8_like = y_num.gte(2013)  # True for L8/L9

        coll = ee.ImageCollection(ee.Algorithms.If(
            y_num.lte(2011),
            ee.ImageCollection('LANDSAT/LT05/C02/T1_L2'),
            ee.Algorithms.If(
                y_num.eq(2012),
                ee.ImageCollection('LANDSAT/LE07/C02/T1_L2'),
                ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
                  .merge(ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'))
            )
        ))

        start = ee.Date.fromYMD(y_num, 8, 1)
        end   = ee.Date.fromYMD(y_num, 12, 31)

        return (coll
                .filterDate(start, end)
                .filterBounds(aoi)
                .map(mask_landsat_clouds)
                .map(scale_sr)
                .map(lambda im: add_ndwi(im, use_l8_like)))

    # --- Build 3-year composite ---
    collection = (get_fall_collection(year.subtract(1))
                  .merge(get_fall_collection(year))
                  .merge(get_fall_collection(year.add(1))))

    # Guard: empty collections
    ndwi_median = ee.Algorithms.If(
        collection.size().gt(0),
        collection.select('NDWI').median().clip(aoi),
        ee.Image.constant(0).rename('NDWI').clip(aoi)
    )
    ndwi_median = ee.Image(ndwi_median)
    water_mask  = ndwi_median.gt(ndwi_thresh)

    # --- Vectorize ---
    lake_fc_all = water_mask.selfMask().reduceToVectors(
        geometry=aoi,
        scale=30,
        geometryType='polygon',
        reducer=ee.Reducer.countEvery(),
        maxPixels=1e13
    )

    def set_area(feat):
        area_ha = feat.geometry().area(maxError=1).divide(1e4)
        return feat.set('Lake_area_calculated_ha', area_ha)

    lake_fc_all = lake_fc_all.map(set_area)

    # --- Pick largest polygon or return point ---
    largest = ee.Feature(ee.Algorithms.If(
        lake_fc_all.size().gt(0),
        lake_fc_all.sort('Lake_area_calculated_ha', False).first(),
        ee.Feature(point, {'Lake_area_calculated_ha': None})
    ))

    return largest






import ee

# 100% server-side function for PRE-GLOF lakes
def detect_lake_pre_glof(point, year, search_radius=3000, ndwi_thresh=0.3):
    """
    Detect PRE-GLOF lake polygons around a point for years leading up to GLOF.
    
    Args:
        point: ee.Geometry.Point
        year: ee.Number or int (Year_final, i.e., event year)
        search_radius: buffer radius around point (m)
        ndwi_thresh: NDWI threshold for water detection
    
    Returns:
        ee.Feature with geometry = largest detected lake polygon (or the point if none)
        property: 'Lake_area_calculated_ha'
    """
    point = ee.Geometry(point)
    year  = ee.Number(year).toInt()
    aoi   = point.buffer(search_radius)

    # --- Cloud mask ---
    def mask_landsat_clouds(img):
        qa = img.select('QA_PIXEL')
        cloud        = 1 << 3
        cloud_shadow = 1 << 4
        snow         = 1 << 5
        mask = (qa.bitwiseAnd(cloud).eq(0)
                  .And(qa.bitwiseAnd(cloud_shadow).eq(0))
                  .And(qa.bitwiseAnd(snow).eq(0)))
        return img.updateMask(mask)

    # --- Scale reflectance ---
    def scale_sr(img):
        optical = img.select('SR_B[0-9]*').multiply(0.0000275).add(-0.2)
        return img.addBands(optical, overwrite=True)

    # --- Add NDWI ---
    def add_ndwi(img, use_l8_like):
        green = ee.Image(ee.Algorithms.If(use_l8_like, img.select('SR_B3'), img.select('SR_B2')))
        nir   = ee.Image(ee.Algorithms.If(use_l8_like, img.select('SR_B5'), img.select('SR_B4')))
        ndwi  = green.subtract(nir).divide(green.add(nir)).rename('NDWI')
        return img.addBands(ndwi)

    # --- Get fall Landsat collection ---
    def get_fall_collection(y_num):
        y_num      = ee.Number(y_num).toInt()
        use_l8_like = y_num.gte(2013)  # True for L8/L9

        coll = ee.ImageCollection(ee.Algorithms.If(
            y_num.lte(2011),
            ee.ImageCollection('LANDSAT/LT05/C02/T1_L2'),
            ee.Algorithms.If(
                y_num.eq(2012),
                ee.ImageCollection('LANDSAT/LE07/C02/T1_L2'),
                ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')
                  .merge(ee.ImageCollection('LANDSAT/LC09/C02/T1_L2'))
            )
        ))

        start = ee.Date.fromYMD(y_num, 8, 1)
        end   = ee.Date.fromYMD(y_num, 12, 31)

        return (coll
                .filterDate(start, end)
                .filterBounds(aoi)
                .map(mask_landsat_clouds)
                .map(scale_sr)
                .map(lambda im: add_ndwi(im, use_l8_like)))

    # --- Build PRE-GLOF 3-year composite (y-3, y-2, y-1) ---
    collection = (get_fall_collection(year.subtract(3))
                  .merge(get_fall_collection(year.subtract(2)))
                  .merge(get_fall_collection(year.subtract(1))))

    # Guard: empty collections
    ndwi_median = ee.Algorithms.If(
        collection.size().gt(0),
        collection.select('NDWI').median().clip(aoi),
        ee.Image.constant(0).rename('NDWI').clip(aoi)
    )
    ndwi_median = ee.Image(ndwi_median)
    water_mask  = ndwi_median.gt(ndwi_thresh)

    # --- Vectorize ---
    lake_fc_all = water_mask.selfMask().reduceToVectors(
        geometry=aoi,
        scale=30,
        geometryType='polygon',
        reducer=ee.Reducer.countEvery(),
        maxPixels=1e13
    )

    def set_area(feat):
        area_ha = feat.geometry().area(maxError=1).divide(1e4)
        return feat.set('Lake_area_calculated_ha', area_ha)

    lake_fc_all = lake_fc_all.map(set_area)

    # --- Pick largest polygon or return point ---
    largest = ee.Feature(ee.Algorithms.If(
        lake_fc_all.size().gt(0),
        lake_fc_all.sort('Lake_area_calculated_ha', False).first(),
        ee.Feature(point, {'Lake_area_calculated_ha': None})
    ))

    return largest



def df_to_fc(df):
    features = []
    for idx, row in df.iterrows():
        point = ee.Geometry.Point([row["Longitude"], row["Latitude"]])

        # Convert row to dict, replace NaN with None
        props = row.where(pd.notnull(row), None).to_dict()
        props["id"] = int(idx)  # unique id for merging later

        features.append(ee.Feature(point, props))
    return ee.FeatureCollection(features)





def detect_area_nonglof(feature):
    point = feature.geometry()
    year  = ee.Number.parse(feature.get("Year_final")).toInt()
    lake  = detect_lake_from_point(point, year)

    # Return the lake feature, with all original properties copied,
    # and explicitly keep the calculated area
    return ee.Feature(lake).copyProperties(feature).set(
        'Lake_area_calculated_ha', lake.get('Lake_area_calculated_ha')
    )




def detect_area_glof(feature):
    point = feature.geometry()
    year  = ee.Number.parse(feature.get("Year_final")).toInt()
    lake  = detect_lake_pre_glof(point, year)

    # Return the lake feature, with all original properties copied,
    # and explicitly keep the calculated area
    return ee.Feature(lake).copyProperties(feature).set(
        'Lake_area_calculated_ha', lake.get('Lake_area_calculated_ha')
    )



pd.set_option('display.max_rows', None)


#split to 2 dataframe:
df_pos = df_pre_enrich[df_pre_enrich["GLOF"] == 1].copy()  # GLOF lakes
df_neg = df_pre_enrich[df_pre_enrich["GLOF"] == 0].copy()  # Non-GLOF lakes



# Test run: compute NDWI-based area (ha) for first 10 negative lakes to verify detect_area_nonglof()
df_neg_50=df_neg.iloc[0:10].copy()
fc = df_to_fc(df_neg_50)
fc_with_area = fc.map(detect_area_nonglof)

data = fc_with_area.getInfo()["features"]
rows = [f["properties"] for f in data]
df_neg_50_area = pd.DataFrame(rows)
df_neg_50_area.head()


# Compute NDWI-based lake area for negative (non-GLOF) lakes in batches with progress tracking
from tqdm import tqdm
import pandas as pd

batch_size = 50
results = []

for i in tqdm(range(0, len(df_neg), batch_size), desc="Processing negative lakes", unit="batch"):
    df_batch = df_neg.iloc[i:i+batch_size]
    fc = df_to_fc(df_batch)
    fc_with_area = fc.map(detect_area_nonglof)

    props = ['id', 'Year_final', 'Lake_area_calculated_ha']
    data = fc_with_area.select(props).getInfo()['features']
    rows = [f['properties'] for f in data]
    results.append(pd.DataFrame(rows))

df_neg_result = pd.concat(results, ignore_index=True)





df_neg_result.head()


df_neg_withcalculatedarea = pd.concat(
    [df_neg.reset_index(drop=True),
     df_neg_result[['Lake_area_calculated_ha']].reset_index(drop=True)],
    axis=1
)
df_neg_withcalculatedarea.head()


df_neg_withcalculatedarea.to_csv("../CSVs/df_neg_withcalculatedarea_ndwi0.3.csv")


#Assigning area to glof lakes:
fc = df_to_fc(df_pos)
fc_with_area = fc.map(detect_area_glof)

data = fc_with_area.getInfo()["features"]
rows = [f["properties"] for f in data]
df_pos_area = pd.DataFrame(rows)
df_pos_area.head()


df_pos_area = df_pos_area.drop(columns=['count','label'])
df_pos_area.head()


df_pos_area.to_csv("../CSVs/df_pos_calculatedarea_ndwi0.3.csv")




















from scipy.stats import spearmanr, kendalltau

# Validation: compare NDWI-calculated vs inventory lake areas (IHR negatives)
df_compare = df_neg_withcalculatedarea.dropna(subset=['Lake_area_ha', 'Lake_area_calculated_ha'])

# Absolute and relative difference
df_compare['abs_diff'] = df_compare['Lake_area_calculated_ha'] - df_compare['Lake_area_ha']
df_compare['rel_diff_pct'] = (df_compare['abs_diff'] / df_compare['Lake_area_ha']) * 100

# Error metrics
mae = np.mean(np.abs(df_compare['abs_diff']))
rmse = np.sqrt(np.mean(df_compare['abs_diff'] ** 2))

# Correlations
pearson_r = df_compare[['Lake_area_ha', 'Lake_area_calculated_ha']].corr(method='pearson').iloc[0, 1]
spearman_r, _ = spearmanr(df_compare['Lake_area_ha'], df_compare['Lake_area_calculated_ha'])
kendall_r, _ = kendalltau(df_compare['Lake_area_ha'], df_compare['Lake_area_calculated_ha'])

print(f"Mean Absolute Error (MAE): {mae:.2f} ha")
print(f"Root Mean Square Error (RMSE): {rmse:.2f} ha")
print(f"Pearson r: {pearson_r:.2f}")
print(f"Spearman ρ: {spearman_r:.2f}")
print(f"Kendall τ: {kendall_r:.2f}")




import matplotlib.pyplot as plt

plt.scatter(df_compare['Lake_area_ha'], df_compare['Lake_area_calculated_ha'], alpha=0.5)
plt.plot([0, 500], [0, 500], 'r--', label='1:1 line')

plt.xlim(0, 200)
plt.ylim(0, 200)

plt.xlabel("Inventory Area (ha)")
plt.ylabel("Calculated Area (ha)")
plt.legend()
plt.title("Calculated vs. Inventory Lake Areas (Zoomed 0–500 ha)")
plt.show()








# Validation of NDWI(0.25)-Derived Lake Areas for Negative Lakes

import pandas as pd
import numpy as np
from scipy.stats import spearmanr, kendalltau
import matplotlib.pyplot as plt

# 2. Load NDWI(0.25) calculated-area data for negatives
df_neg_val = pd.read_csv("../CSVs/df_neg_withcalculatedarea_ndwi0.25.csv")

df_neg_val = df_neg_val.dropna(subset=['Lake_area_ha', 'Lake_area_calculated_ha'])

# 3. Compute difference and error metrics
df_neg_val['abs_diff'] = df_neg_val['Lake_area_calculated_ha'] - df_neg_val['Lake_area_ha']
df_neg_val['rel_diff_pct'] = (df_neg_val['abs_diff'] / df_neg_val['Lake_area_ha']) * 100

mae = np.mean(np.abs(df_neg_val['abs_diff']))
rmse = np.sqrt(np.mean(df_neg_val['abs_diff'] ** 2))

# 4. Compute correlations
pearson_r = df_neg_val[['Lake_area_ha','Lake_area_calculated_ha']].corr(method='pearson').iloc[0,1]
spearman_r, _ = spearmanr(df_neg_val['Lake_area_ha'], df_neg_val['Lake_area_calculated_ha'])
kendall_r, _ = kendalltau(df_neg_val['Lake_area_ha'], df_neg_val['Lake_area_calculated_ha'])

# 5. Print validation metrics
print("=== NDWI (0.25) Validation for Negative Lakes ===")
print(f"Mean Absolute Error (MAE): {mae:.2f} ha")
print(f"Root Mean Square Error (RMSE): {rmse:.2f} ha")
print(f"Pearson r: {pearson_r:.2f}")
print(f"Spearman ρ: {spearman_r:.2f}")
print(f"Kendall τ: {kendall_r:.2f}")

# 6. Simple scatter: Calculated vs Inventory (IHR negatives)
plt.figure(figsize=(6,6))
plt.scatter(df_neg_val['Lake_area_ha'], df_neg_val['Lake_area_calculated_ha'],
            alpha=0.6, color='steelblue', label='Lakes')
plt.plot([0, 500], [0, 500], 'r--', label='1:1 line')

plt.xlim(0, 200)
plt.ylim(0, 200)
plt.xlabel("Inventory Area (ha)")
plt.ylabel("Calculated Area (ha)")
plt.title("Calculated vs. Inventory Lake Areas (Negatives, NDWI = 0.25)")
plt.legend(frameon=False)
plt.tight_layout()
plt.show()















def calc_expansion_glof(feature):
    point = feature.geometry()
    year = ee.Number.parse(feature.get("Year_final")).toInt()
    years = ee.Number(5)

    # Detect lakes
    lake_t1 = detect_lake_pre_glof(point, year.subtract(5))
    lake_t2 = detect_lake_pre_glof(point, year)

    # Get raw areas
    area_t1_raw = lake_t1.get("Lake_area_calculated_ha")
    area_t2_raw = lake_t2.get("Lake_area_calculated_ha")

    # Absolute expansion (only if both areas exist)
    rate_abs = ee.Algorithms.If(
        ee.Algorithms.IsEqual(area_t1_raw, None),
        None,
        ee.Algorithms.If(
            ee.Algorithms.IsEqual(area_t2_raw, None),
            None,
            ee.Number(area_t2_raw).subtract(ee.Number(area_t1_raw)).divide(years)
        )
    )

    return (ee.Feature(lake_t2.geometry())
        .set("Latitude", feature.get("Latitude"))
        .set("Longitude", feature.get("Longitude"))
        .set("Year_final", year)
        .set("GLOF", feature.get("GLOF"))
        .set("area_t1", area_t1_raw)
        .set("area_t2", area_t2_raw)
        .set("expansion_ha_peryr", rate_abs))



def calc_expansion_nonglof(feature):
    point = feature.geometry()
    year = ee.Number.parse(feature.get("Year_final")).toInt()
    years = ee.Number(5)

    lake_t1 = detect_lake_from_point(point, year.subtract(5))
    lake_t2 = detect_lake_from_point(point, year)

    area_t1_raw = lake_t1.get("Lake_area_calculated_ha")
    area_t2_raw = lake_t2.get("Lake_area_calculated_ha")

    rate_abs = ee.Algorithms.If(
        ee.Algorithms.IsEqual(area_t1_raw, None),
        None,
        ee.Algorithms.If(
            ee.Algorithms.IsEqual(area_t2_raw, None),
            None,
            ee.Number(area_t2_raw).subtract(ee.Number(area_t1_raw)).divide(years)
        )
    )

    return (ee.Feature(lake_t2.geometry())
        .set("Latitude", feature.get("Latitude"))
        .set("Longitude", feature.get("Longitude"))
        .set("Year_final", year)
        .set("GLOF", feature.get("GLOF"))
        .set("area_t1", area_t1_raw)
        .set("area_t2", area_t2_raw)
        .set("expansion_ha_peryr", rate_abs))




fc_pos = df_to_fc(df_pos)

# --- Apply expansion calculation ---
fc_pos_expansion = fc_pos.map(calc_expansion_glof)

# --- Pull results back to pandas ---
result_pos = fc_pos_expansion.getInfo()["features"]

rows = []
for feat in result_pos:
    props = feat["properties"]
    rows.append(props)

df_pos_expansion = pd.DataFrame(rows)


df_pos_expansion.head()


df_pos_expansion5y=df_pos_expansion
df_pos_expansion5y.head()
df_pos_expansion5y.to_csv("pos_expansion5y.csv")



#adding neg_expansion5y


batch_size = 50
results = []

for i in range(0, len(df_neg_withcalculatedarea), batch_size):
    df_batch = df_neg_withcalculatedarea.iloc[i:i+batch_size]
    fc = df_to_fc(df_batch)

    # Apply expansion calculation
    fc_with_expansion = fc.map(calc_expansion_nonglof)

    # Pull properties back (including expansion)
    data = fc_with_expansion.getInfo()["features"]
    rows = [f["properties"] for f in data]
    results.append(pd.DataFrame(rows))

# Combine batches
df_expansion_updates = pd.concat(results, ignore_index=True)

# --- Merge with your original df (align by Latitude, Longitude, Year_final) ---
df_neg_expansion = pd.merge(
    df_neg_withcalculatedarea,
    df_expansion_updates[["Latitude", "Longitude", "Year_final",
                          "area_t1", "area_t2", "expansion_ha_peryr"]],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)




df_neg_expansion.head()


df_neg_expansion5y=df_neg_expansion
df_neg_expansion5y.head()



df_neg_expansion5y.to_csv("neg_expansion5y.csv")





def calc_expansion_glof(feature):
    point = feature.geometry()
    year = ee.Number.parse(feature.get("Year_final")).toInt()
    years = ee.Number(10)

    # Detect lakes
    lake_t1 = detect_lake_pre_glof(point, year.subtract(10))
    lake_t2 = detect_lake_pre_glof(point, year)

    # Get raw areas
    area_t1_raw = lake_t1.get("Lake_area_calculated_ha")
    area_t2_raw = lake_t2.get("Lake_area_calculated_ha")

    # Absolute expansion (only if both areas exist)
    rate_abs = ee.Algorithms.If(
        ee.Algorithms.IsEqual(area_t1_raw, None),
        None,
        ee.Algorithms.If(
            ee.Algorithms.IsEqual(area_t2_raw, None),
            None,
            ee.Number(area_t2_raw).subtract(ee.Number(area_t1_raw)).divide(years)
        )
    )

    return (ee.Feature(lake_t2.geometry())
        .set("Latitude", feature.get("Latitude"))
        .set("Longitude", feature.get("Longitude"))
        .set("Year_final", year)
        .set("GLOF", feature.get("GLOF"))
        .set("area_t1", area_t1_raw)
        .set("area_t2", area_t2_raw)
        .set("expansion_ha_peryr", rate_abs))
    
def calc_expansion_nonglof(feature):
    point = feature.geometry()
    year = ee.Number.parse(feature.get("Year_final")).toInt()
    years = ee.Number(10)

    lake_t1 = detect_lake_from_point(point, year.subtract(10))
    lake_t2 = detect_lake_from_point(point, year)

    area_t1_raw = lake_t1.get("Lake_area_calculated_ha")
    area_t2_raw = lake_t2.get("Lake_area_calculated_ha")

    rate_abs = ee.Algorithms.If(
        ee.Algorithms.IsEqual(area_t1_raw, None),
        None,
        ee.Algorithms.If(
            ee.Algorithms.IsEqual(area_t2_raw, None),
            None,
            ee.Number(area_t2_raw).subtract(ee.Number(area_t1_raw)).divide(years)
        )
    )

    return (ee.Feature(lake_t2.geometry())
        .set("Latitude", feature.get("Latitude"))
        .set("Longitude", feature.get("Longitude"))
        .set("Year_final", year)
        .set("GLOF", feature.get("GLOF"))
        .set("area_t1", area_t1_raw)
        .set("area_t2", area_t2_raw)
        .set("expansion_ha_peryr", rate_abs))







import pandas as pd
df_neg_expansion5y=pd.read_csv("neg_expansion5y.csv")


batch_size = 50
results = []

for i in range(0, len(df_neg_expansion5y), batch_size):
    df_batch = df_neg_expansion5y.iloc[i:i+batch_size]
    fc = df_to_fc(df_batch)

    # Apply expansion calculation
    fc_with_expansion10y = fc.map(calc_expansion_nonglof)

    # Pull properties back (including expansion)
    data = fc_with_expansion10y.getInfo()["features"]
    rows = [f["properties"] for f in data]
    results.append(pd.DataFrame(rows))

# Combine batches
df_expansion_updates = pd.concat(results, ignore_index=True)





df_neg_withcalculatedarea=pd.read_csv("df_neg_withcalculatedarea.csv")
df_neg_withcalculatedarea.head()


# --- Merge with your original df (align by Latitude, Longitude, Year_final) ---
df_neg_expansion10y = pd.merge(
    df_neg_withcalculatedarea,
    df_expansion_updates[["Latitude", "Longitude", "Year_final",
                          "area_t1", "area_t2", "expansion_ha_peryr"]],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)



df_neg_expansion10y.to_csv("neg_expansion10y.csv")





df_pos_calculatedarea=pd.read_csv("df_pos_calculatedarea.csv")
df_pos_calculatedarea.head()


fc_pos_calculatedarea = df_to_fc(df_pos_calculatedarea)

# --- Apply expansion calculation ---
fc_pos_expansion10y = fc_pos_calculatedarea.map(calc_expansion_glof)

# --- Pull results back to pandas ---
result_pos= fc_pos_expansion10y.getInfo()["features"]

rows = []
for feat in result_pos:
    props = feat["properties"]
    rows.append(props)

df_pos_expansion10y = pd.DataFrame(rows)


df_pos_expansion10y.head()


df_pos_expansion10y.to_csv("pos_expansion10y.csv")








## From Validation to Prediction: Updating Non-GLOF Lake Features to 2024

import pandas as pd
from tqdm import tqdm

# --- Load the base dataset ---
df_neg = pd.read_csv("../CSVs/df_neg_withcalculatedarea_ndwi0.3.csv")

# --- Set all Year_final values to 2023 ---
df_neg['Year_final'] = 2023

# --- Compute NDWI-based lake area for non-GLOF lakes ---
batch_size = 50
results = []

for i in tqdm(range(0, len(df_neg), batch_size), desc="Processing negative lakes", unit="batch"):
    df_batch = df_neg.iloc[i:i + batch_size]
    fc = df_to_fc(df_batch)
    fc_with_area = fc.map(detect_area_nonglof)

    props = ['id', 'Year_final', 'Lake_area_calculated_ha']
    data = fc_with_area.select(props).getInfo()['features']
    rows = [f['properties'] for f in data]
    results.append(pd.DataFrame(rows))

# Combine batch results
df_neg_result2023 = pd.concat(results, ignore_index=True)

# --- Merge new calculated areas back into the main dataframe ---
df_neg_withcalculatedarea2023 = pd.concat(
    [
        df_neg.reset_index(drop=True),
        df_neg_result2023[['Lake_area_calculated_ha']].reset_index(drop=True)
    ],
    axis=1
)



df_neg_withcalculatedarea2023.head()


# --- Merge new calculated areas back into the main dataframe ---
df_neg_withcalculatedarea2023 = pd.concat(
    [
        df_neg.reset_index(drop=True),
        df_neg_result2023[['Lake_area_calculated_ha']].reset_index(drop=True)
    ],
    axis=1
)



# --- Save the updated dataframe ---
df_neg_withcalculatedarea2023.to_csv(output_path, index=False)

print(" Saved updated lake features /CSVs/df_neg_calc_area2023_ndwi0.3.csv")
df_neg_withcalculatedarea2023.head()


# Remove duplicate column, keep the last one(2023 area)
df_neg_withcalculatedarea2023 = df_neg_withcalculatedarea2023.loc[:, ~df_neg_withcalculatedarea2023.columns.duplicated(keep='last')]
df_neg_withcalculatedarea2023.head()


df_neg_withcalculatedarea2023.to_csv("../CSVs/df_neg_calc_area2023_ndwi0.3.csv", index=False)





# 5years
def calc_expansion_nonglof(feature):
    point = feature.geometry()
    year = ee.Number.parse(feature.get("Year_final")).toInt()
    years = ee.Number(5)

    lake_t1 = detect_lake_from_point(point, year.subtract(5))
    lake_t2 = detect_lake_from_point(point, year)

    area_t1_raw = lake_t1.get("Lake_area_calculated_ha")
    area_t2_raw = lake_t2.get("Lake_area_calculated_ha")

    rate_abs = ee.Algorithms.If(
        ee.Algorithms.IsEqual(area_t1_raw, None),
        None,
        ee.Algorithms.If(
            ee.Algorithms.IsEqual(area_t2_raw, None),
            None,
            ee.Number(area_t2_raw).subtract(ee.Number(area_t1_raw)).divide(years)
        )
    )

    return (ee.Feature(lake_t2.geometry())
        .set("Latitude", feature.get("Latitude"))
        .set("Longitude", feature.get("Longitude"))
        .set("Year_final", year)
        .set("GLOF", feature.get("GLOF"))
        .set("area_t1", area_t1_raw)
        .set("area_t2", area_t2_raw)
        .set("expansion_ha_peryr", rate_abs))


## Add 5-year Expansion Columns for Non-GLOF Lakes (2023 version)

import pandas as pd
from tqdm import tqdm

# Load updated 2023 NDWI-derived lake dataset
df_neg_area2023 = pd.read_csv("../CSVs/df_neg_calc_area2023_ndwi0.3.csv")

batch_size = 50
expansion_results_2023 = []

# Run expansion calculation in batches
for i in tqdm(range(0, len(df_neg_area2023), batch_size), desc="Computing 5-year expansion", unit="batch"):
    df_batch = df_neg_area2023.iloc[i:i + batch_size]
    fc = df_to_fc(df_batch)
    fc_with_expansion = fc.map(calc_expansion_nonglof)
    data = fc_with_expansion.getInfo()["features"]
    rows = [f["properties"] for f in data]
    expansion_results_2023.append(pd.DataFrame(rows))

# Combine and merge results with main 2023 dataframe
df_expansion_updates_2023 = pd.concat(expansion_results_2023, ignore_index=True)

df_neg_expansion5y_2023 = pd.merge(
    df_neg_area2023,
    df_expansion_updates_2023[
        ["Latitude", "Longitude", "Year_final", "area_t1", "area_t2", "expansion_ha_peryr"]
    ],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)

df_neg_expansion5y_2023.head()




# Save final 2023 expansion file
output_path = "../CSVs/df_neg_expansion5y_2023.csv"
df_neg_expansion5y_2023.to_csv(output_path, index=False)

print(f"Saved 5-year expansion results to: {output_path}")


def calc_expansion10y_nonglof(feature):
    point = feature.geometry()
    year = ee.Number.parse(feature.get("Year_final")).toInt()
    years = ee.Number(10)

    lake_t1 = detect_lake_from_point(point, year.subtract(10))
    lake_t2 = detect_lake_from_point(point, year)

    area_t1_raw = lake_t1.get("Lake_area_calculated_ha")
    area_t2_raw = lake_t2.get("Lake_area_calculated_ha")

    rate_abs = ee.Algorithms.If(
        ee.Algorithms.IsEqual(area_t1_raw, None),
        None,
        ee.Algorithms.If(
            ee.Algorithms.IsEqual(area_t2_raw, None),
            None,
            ee.Number(area_t2_raw).subtract(ee.Number(area_t1_raw)).divide(years)
        )
    )

    return (ee.Feature(lake_t2.geometry())
        .set("Latitude", feature.get("Latitude"))
        .set("Longitude", feature.get("Longitude"))
        .set("Year_final", year)
        .set("GLOF", feature.get("GLOF"))
        .set("area_t1_10y", area_t1_raw)
        .set("area_t2_10y", area_t2_raw)
        .set("expansion_ha_peryr_10y", rate_abs))



## Add 10-year Expansion Columns for Non-GLOF Lakes (2023 version)

import pandas as pd
from tqdm import tqdm

# Load the 2023 NDWI-derived lake dataset
df_neg_area2023 = pd.read_csv("../CSVs/df_neg_calc_area2023_ndwi0.3.csv")

batch_size = 50
expansion_results10y_2023 = []

# Run 10-year expansion calculation in batches
for i in tqdm(range(0, len(df_neg_area2023), batch_size), desc="Computing 10-year expansion", unit="batch"):
    df_batch = df_neg_area2023.iloc[i:i + batch_size]
    fc = df_to_fc(df_batch)
    fc_with_expansion10y = fc.map(calc_expansion10y_nonglof)
    data = fc_with_expansion10y.getInfo()["features"]
    rows = [f["properties"] for f in data]
    expansion_results10y_2023.append(pd.DataFrame(rows))

# Combine and merge results with main 2023 dataframe
df_expansion10y_updates_2023 = pd.concat(expansion_results10y_2023, ignore_index=True)

df_neg_expansion10y_2023 = pd.merge(
    df_neg_area2023,
    df_expansion10y_updates_2023[
        ["Latitude", "Longitude", "Year_final",
         "area_t1_10y", "area_t2_10y", "expansion_ha_peryr_10y"]
    ],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)

df_neg_expansion10y_2023.head()



# Save final 10-year expansion file
output_path = "../CSVs/df_neg_expansion10y_2023.csv"
df_neg_expansion10y_2023.to_csv(output_path, index=False)

print(f"Saved 10-year expansion results to: {output_path}")








import pandas as pd

# Load base and expansion datasets
df_neg_area2023 = pd.read_csv("../CSVs/df_neg_calc_area2023_ndwi0.3.csv")
df_5y = pd.read_csv("../CSVs/df_neg_expansion5y_2023.csv")
df_10y = pd.read_csv("../CSVs/df_neg_expansion10y_2023.csv")

# Add expansion columns to base dataframe (same order assumption)
df_neg_2023_pre_glacierfeatures = df_neg_area2023.copy()
df_neg_2023_pre_glacierfeatures["expansion_ha_peryr_5y"] = df_5y["expansion_ha_peryr"]
df_neg_2023_pre_glacierfeatures["expansion_ha_peryr_10y"] = df_10y["expansion_ha_peryr_10y"]

df_neg_2023_pre_glacierfeatures.head()



# Save final combined file
output_path = "../CSVs/df_neg_2023_pre_glacierfeatures.csv"
df_neg_2023_pre_glacierfeatures.to_csv(output_path, index=False)

print(f"Combined dataset saved to: {output_path}")


## Update Non-GLOF Lake Features Only (Keep GLOF=1 intact)

import pandas as pd

# Load combined ML dataset (positives + negatives)
df_ml = pd.read_csv("../CSVs/uncleaned_ml_combined.csv")

# Load validated & updated datasets for non-GLOF lakes
df_area2023 = pd.read_csv("../CSVs/df_neg_calc_area2023_ndwi0.3.csv")
df_5y = pd.read_csv("../CSVs/df_neg_expansion5y_2023.csv")
df_10y = pd.read_csv("../CSVs/df_neg_expansion10y_2023.csv")

# Drop outdated columns
cols_to_remove = ["Lake_area_calculated_ha", "5y_expansion_rate", "10y_expansion_rate", "Year_final"]
df_ml = df_ml.drop(columns=cols_to_remove, errors="ignore")

# --- Copy dataframe to preserve structure ---
df_updated = df_ml.copy()

# --- Identify negative (non-GLOF) lakes ---
mask_neg = df_updated["GLOF"] == 0

# --- Replace only for non-GLOF rows ---
df_updated.loc[mask_neg, "Lake_area_calculated_ha"] = df_area2023["Lake_area_calculated_ha"].values
df_updated.loc[mask_neg, "expansion_ha_peryr_5y"] = df_5y["expansion_ha_peryr"].values
df_updated.loc[mask_neg, "expansion_ha_peryr_10y"] = df_10y["expansion_ha_peryr_10y"].values

# --- Save updated combined dataset ---
output_path = "../CSVs/ml_combined_updated2023.csv"
df_updated.to_csv(output_path, index=False)

print(f"Saved updated combined dataset → {output_path}")
df_updated.head()



df_updated.head(10)











#gets the nearest glacier polygons to lake "polygon" and if not touch just gets the closest glacier's info: not fully server side


def get_nearest_glacier(lake_geom, buffer_km=50):
    """
    Given a lake polygon, finds nearest or touching glacier(s).
    Returns consistent features for ML:
      - glacier_contact (bool)
      - nearest_glacier_dist_m
      - lake_elev_m
      - glacier_elev_m (from chosen glacier)
      - slope_glac_to_lake (rise/run, slope=0 if touching)
      - glacier_area_km2 (sum of all touching glaciers, or area of nearest one)
      - glacier_ids (list of GLIMS IDs for touching glaciers, or [nearest_id])
    """
    glaciers = ee.FeatureCollection("GLIMS/current")
    dem = ee.Image("USGS/SRTMGL1_003")

    # Filter glaciers within buffer
    nearby = glaciers.filterBounds(lake_geom.buffer(buffer_km * 1000))

    # Glaciers that touch/intersect the lake
    touching = nearby.filterBounds(lake_geom)

    # --- Case 1: touching glaciers ---
    if touching.size().getInfo() > 0:
        # Add area for each
        def add_area(f):
            return f.set("glacier_area_km2", f.geometry().area(maxError=30).divide(1e6))
        touching = touching.map(add_area)

        # Glacier with largest area (used for slope reference)
        largest_touching = ee.Feature(touching.sort("glacier_area_km2", False).first())

        # Collect all glacier IDs
        glacier_ids = touching.aggregate_array("glac_id").getInfo()

        # Metrics
        combined_area = touching.aggregate_sum("glacier_area_km2").getInfo()
        lake_elev = dem.sample(lake_geom.centroid(maxError=30), 30).first().get("elevation").getInfo()
        glac_elev = dem.sample(largest_touching.geometry().centroid(maxError=30), 30).first().get("elevation").getInfo()

        return {
            "glacier_contact": True,
            "nearest_glacier_dist_m": 0,   # touching → distance = 0
            "lake_elev_m": lake_elev,
            "glacier_elev_m": glac_elev,
            "slope_glac_to_lake": 0,      # standardized → slope=0 when touching
            "glacier_area_km2": combined_area,   # sum of all touching glaciers
            "glacier_ids": glacier_ids    # all touching glaciers
        }

    # --- Case 2: no touching glaciers ---
    else:
        # Compute distances
        with_dist = nearby.map(lambda g: g.set("dist", g.geometry().distance(lake_geom, maxError=30)))
        nearest = ee.Feature(with_dist.sort("dist").first())

        # Metrics
        distance = nearest.get("dist").getInfo()
        area_km2 = nearest.geometry().area(maxError=30).divide(1e6).getInfo()
        lake_elev = dem.sample(lake_geom.centroid(maxError=30), 30).first().get("elevation").getInfo()
        glac_elev = dem.sample(nearest.geometry().centroid(maxError=30), 30).first().get("elevation").getInfo()

        slope = (glac_elev - lake_elev) / distance if (lake_elev and glac_elev and distance > 0) else None

        return {
            "glacier_contact": False,
            "nearest_glacier_dist_m": distance,
            "lake_elev_m": lake_elev,
            "glacier_elev_m": glac_elev,
            "slope_glac_to_lake": slope,
            "glacier_area_km2": area_km2,
            "glacier_ids": [nearest.get("glac_id").getInfo()]  # single nearest glacier
        }








#visualize glacier features:


def visualize_glacierE(lake_geom, glacier_info, zoom=11):
    """
    Visualize the lake polygon, glaciers (touching or nearest), and connection line.
    Compatible with the new get_nearest_glacier() output.
    """
    # Base map
    center = lake_geom.centroid(maxError=30).coordinates().getInfo()[::-1]  # [lat, lon]
    m = geemap.Map(center=center, zoom=zoom)
    m.add_basemap("SATELLITE")

    # DEM overlay (SRTM 30m)
    dem = ee.Image("USGS/SRTMGL1_003")
    dem_vis = {
        "min": 4000,
        "max": 6000,
        "palette": ["blue", "green", "yellow", "orange", "red", "white"]
    }
    m.addLayer(dem, dem_vis, "DEM Elevation")

    # Lake polygon
    m.addLayer(lake_geom, {"color": "cyan"}, "Lake Polygon")

    if glacier_info["glacier_contact"]:a
        # --- Case 1: Touching glaciers ---
        largest_overlap = ee.Feature(glacier_info["largest_overlap_glacier"])
        m.addLayer(largest_overlap.geometry(), {"color": "purple"}, "Largest Overlap Glacier")

    else:
        # --- Case 2: Nearest glacier ---
        nearest = ee.Feature(glacier_info["nearest_glacier"])
        nearest_point = nearest.geometry().centroid(maxError=30)
        lake_centroid = lake_geom.centroid(maxError=30)

        # Buffers
        m.addLayer(lake_centroid.buffer(200), {"color": "black"}, "Lake Centroid")
        m.addLayer(nearest_point.buffer(100), {"color": "red"}, "Nearest Glacier Point")

        # Glacier polygon
        m.addLayer(nearest.geometry(), {"color": "purple"}, "Nearest Glacier")

        # Connection line
        line = ee.Geometry.LineString([
            lake_centroid.coordinates(),
            nearest_point.coordinates()
        ])
        m.addLayer(line, {"color": "yellow", "width": 2}, "Connection Line")

    return m





# --- Test one lake ---
lat, lon = 27.864,87.837   # example coords
point = ee.Geometry.Point([lon, lat])

# 1. Detect lake polygon
lake_result = detect_lake_from_point(point, 2016)
lake_fc = ee.FeatureCollection(lake_result["lake_fc"])
print("Lake polygons detected:", lake_fc.size().getInfo())

if lake_fc.size().getInfo() > 0:
    lake_geom = ee.Feature(lake_fc.first()).geometry(maxError=30)
    print("Lake area (km²):", lake_geom.area(maxError=30).divide(1e6).getInfo())

    # 2. Run glacier analysis
    glacier_info = get_nearest_glacier(lake_geom, buffer_km=50)


else:
    print("No lake detected for this point/year.")



lat,lon=36.145,73.639
point = ee.Geometry.Point([lon, lat])   # Earth Engine Point, order = [lon, lat]

lake_result = detect_lake_from_point(point, 2016)


# This is a FeatureCollection

lake_fc = ee.FeatureCollection(lake_result["lake_fc"])
lake_geom = ee.Feature(lake_fc.first()).geometry()

m = visualize_glacierE(lake_geom, glacier_info, zoom=11)
m






import ee, geemap

# --- Glacier analysis function ---
def get_nearest_glacier(lake_geom, buffer_km=50):
    """
    Given a lake polygon, finds nearest or touching glacier(s).
    Returns features for ML and visualization:
      - glacier_contact (bool)
      - nearest_glacier_dist_m
      - lake_elev_m
      - glacier_elev_m (from chosen glacier)
      - slope_glac_to_lake (rise/run, slope=0 if touching)
      - glacier_area_km2 (sum of all touching glaciers, or area of nearest one)
      - glacier_ids (list of GLIMS IDs for touching glaciers, or [nearest_id])
      - largest_overlap_glacier (ee.Feature, only if contact=True)
      - nearest_glacier (ee.Feature, only if contact=False)
    """
    glaciers = ee.FeatureCollection("GLIMS/current")
    dem = ee.Image("USGS/SRTMGL1_003")

    # Filter glaciers within buffer
    nearby = glaciers.filterBounds(lake_geom.buffer(buffer_km * 1000))

    # Glaciers that touch/intersect the lake
    touching = nearby.filterBounds(lake_geom)

    # --- Case 1: touching glaciers ---
    if touching.size().getInfo() > 0:
        # Add area for each
        def add_area(f):
            return f.set("glacier_area_km2", f.geometry().area(maxError=30).divide(1e6))
        touching = touching.map(add_area)

        # Glacier with largest area
        largest_touching = ee.Feature(touching.sort("glacier_area_km2", False).first())

        # Collect all glacier IDs
        glacier_ids = touching.aggregate_array("glac_id").getInfo()

        # Metrics
        combined_area = touching.aggregate_sum("glacier_area_km2").getInfo()
        lake_elev = dem.sample(lake_geom.centroid(maxError=30), 30).first().get("elevation").getInfo()
        glac_elev = dem.sample(largest_touching.geometry().centroid(maxError=30), 30).first().get("elevation").getInfo()

        return {
            "glacier_contact": True,
            "nearest_glacier_dist_m": 0,
            "lake_elev_m": lake_elev,
            "glacier_elev_m": glac_elev,
            "slope_glac_to_lake": 0,
            "glacier_area_km2": combined_area,
            "glacier_ids": glacier_ids,
            "largest_overlap_glacier": largest_touching,
            "nearest_glacier": None
        }

    # --- Case 2: no touching glaciers ---
    else:
        # Compute distances
        with_dist = nearby.map(lambda g: g.set("dist", g.geometry().distance(lake_geom, maxError=30)))
        nearest = ee.Feature(with_dist.sort("dist").first())

        # Metrics
        distance = nearest.get("dist").getInfo()
        area_km2 = nearest.geometry().area(maxError=30).divide(1e6).getInfo()
        lake_elev = dem.sample(lake_geom.centroid(maxError=30), 30).first().get("elevation").getInfo()
        glac_elev = dem.sample(nearest.geometry().centroid(maxError=30), 30).first().get("elevation").getInfo()

        slope = (glac_elev - lake_elev) / distance if (lake_elev and glac_elev and distance > 0) else None

        return {
            "glacier_contact": False,
            "nearest_glacier_dist_m": distance,
            "lake_elev_m": lake_elev,
            "glacier_elev_m": glac_elev,
            "slope_glac_to_lake": slope,
            "glacier_area_km2": area_km2,
            "glacier_ids": [nearest.get("glac_id").getInfo()],
            "largest_overlap_glacier": None,
            "nearest_glacier": nearest
        }


# --- Visualization function (works with the above output) ---
def visualize_glacierE(lake_geom, glacier_info, zoom=11):
    """
    Visualize the lake polygon, glaciers (touching or nearest), and connection line.
    Compatible with the new get_nearest_glacier() output.
    """
    # Base map
    center = lake_geom.centroid(maxError=30).coordinates().getInfo()[::-1]  # [lat, lon]
    m = geemap.Map(center=center, zoom=zoom)
    m.add_basemap("SATELLITE")

    # DEM overlay
    dem = ee.Image("USGS/SRTMGL1_003")
    dem_vis = {"min": 4000, "max": 6000, "palette": ["blue", "green", "yellow", "orange", "red", "white"]}
    m.addLayer(dem, dem_vis, "DEM Elevation")

    # Lake polygon
    m.addLayer(lake_geom, {"color": "cyan"}, "Lake Polygon")

    if glacier_info["glacier_contact"]:
        # --- Case 1: Touching glaciers ---
        largest_overlap = ee.Feature(glacier_info["largest_overlap_glacier"])
        m.addLayer(largest_overlap.geometry(), {"color": "purple"}, "Largest Overlap Glacier")

    else:
        # --- Case 2: Nearest glacier ---
        nearest = ee.Feature(glacier_info["nearest_glacier"])
        nearest_point = nearest.geometry().centroid(maxError=30)
        lake_centroid = lake_geom.centroid(maxError=30)

        m.addLayer(lake_centroid.buffer(200), {"color": "black"}, "Lake Centroid")
        m.addLayer(nearest_point.buffer(100), {"color": "red"}, "Nearest Glacier Point")
        m.addLayer(nearest.geometry(), {"color": "purple"}, "Nearest Glacier")

        line = ee.Geometry.LineString([
            lake_centroid.coordinates(),
            nearest_point.coordinates()
        ])
        m.addLayer(line, {"color": "yellow", "width": 2}, "Connection Line")
        # ✅ Add scale bar (km reference as you zoom)
        m.add_child(geemap.ScaleBar(position="bottomleft"))

    return m



lat, lon = 28.283,86.502
point = ee.Geometry.Point([lon, lat])

lake_result = detect_lake_from_point(point, 2016)
lake_fc = ee.FeatureCollection(lake_result["lake_fc"])

lake_geom = ee.Feature(lake_fc.first()).geometry()
glacier_info = get_nearest_glacier(lake_geom, buffer_km=50)
m = visualize_glacierE(lake_geom, glacier_info)
m  # display map



lat, lon = 35.016,75.001
point = ee.Geometry.Point([lon, lat])

lake_result = detect_lake_from_point(point, 2016)
lake_fc = ee.FeatureCollection(lake_result["lake_fc"])

lake_geom = ee.Feature(lake_fc.first()).geometry()
glacier_info = get_nearest_glacier(lake_geom, buffer_km=50)
m = visualize_glacierE(lake_geom, glacier_info)
m  # display map














import ee, pandas as pd

# --- 1) Core: annotate ONE lake polygon with glacier metrics (all server-side) ---
def _glacier_metrics_for_lake_polygon(lake, buffer_km=50):
    glaciers = ee.FeatureCollection("GLIMS/current")
    dem      = ee.Image("USGS/SRTMGL1_003")

    # Ensure non-zero-area geometry (handles accidental points)
    lake_geom = ee.Feature(lake).geometry().buffer(1)

    # Candidate glaciers within search buffer (meters)
    search = lake_geom.buffer(ee.Number(buffer_km).multiply(1000))
    nearby = glaciers.filterBounds(search)

    # If no glaciers nearby → return null-like fields
    def _no_hits(f):
        return ee.Feature(f).set({
            "glacier_contact": False,
            "glacier_touch_count": 0,
            "nearest_glacier_dist_m": None,
            "lake_elev_m": None,
            "glacier_elev_m": None,
            "slope_glac_to_lake": None,
            "glacier_area_ha": None,
            "glacier_ids": ee.List([]),
            "buffer_km_used": buffer_km,
            "slope_method": "min_segment_poly_to_poly"
        })

    # If glaciers found → compute metrics
    def _compute(f):
        # Per-glacier metrics: distance and area (in hectares)
        def add_metrics(g):
            gg = ee.Feature(g).geometry()
            return ee.Feature(g).set({
                "dist": gg.distance(lake_geom, 1),            # meters (0 if overlap/touch)
                "glacier_area_ha": gg.area(1).divide(1e4)     # hectares
            })
        with_metrics = nearby.map(add_metrics)

        # Nearest glacier by poly↔poly min distance
        nearest  = ee.Feature(with_metrics.sort("dist").first())
        min_dist = ee.Number(nearest.get("dist"))
ke
        # Touching glaciers (dist == 0)
        touching       = with_metrics.filter(ee.Filter.eq("dist", 0))
        touch_ids_list = ee.List(touching.aggregate_array("glac_id")).distinct()
        touch_count    = touch_ids_list.length()

        # Total area if multiple touching glaciers
        touch_area = ee.Algorithms.If(
            touch_count.gt(0),
            ee.Number(touching.aggregate_sum("glacier_area_ha")),
            ee.Number(0)
        )

        # Closest points for elevation sampling
        nearest_geom = nearest.geometry()
        lake_near_pt = lake_geom.closestPoint(nearest_geom, 1)
        glac_near_pt = nearest_geom.closestPoint(lake_geom, 1)

        lake_elev = dem.sample(lake_near_pt, 30).first().get("elevation")
        glac_elev = dem.sample(glac_near_pt, 30).first().get("elevation")

        # Slope along the min-distance line; 0 if touching/overlap
        slope = ee.Algorithms.If(
            min_dist.gt(0),
            ee.Number(glac_elev).subtract(ee.Number(lake_elev)).divide(min_dist),
            ee.Number(0)
        )

        # Use summed area/IDs if touching; else nearest glacier’s stats
        area_choice = ee.Algorithms.If(touch_count.gt(0), touch_area, nearest.get("glacier_area_ha"))
        ids_choice  = ee.Algorithms.If(touch_count.gt(0), touch_ids_list, ee.List([nearest.get("glac_id")]))

        return ee.Feature(f).set({
            "glacier_contact":        touch_count.gt(0),
            "glacier_touch_count":    touch_count,
            "nearest_glacier_dist_m": min_dist,         # 0 if touching/overlap
            "lake_elev_m":            lake_elev,        # at closest point on lake polygon
            "glacier_elev_m":         glac_elev,        # at closest point on glacier polygon
            "slope_glac_to_lake":     slope,            # 0 if touching/overlap
            "glacier_area_ha":        area_choice,      # hectares (sum if multiple touching)
            "glacier_ids":            ids_choice,
            "buffer_km_used":         buffer_km,
            "slope_method":           "min_segment_poly_to_poly"
        })

    return ee.Feature(
        ee.Algorithms.If(nearby.size().eq(0), _no_hits(lake), _compute(lake))
    )
# --- 2) Public: map over a FeatureCollection whose geometry is a LAKE POLYGON (or point) ---
def annotate_glacier_metrics_from_polygons(lakes_fc, buffer_km=50):
    """
    Input:  FeatureCollection where geometry is the LAKE polygon.
            (If some are points, they are buffered 1 m internally so distance works.)
    Output: Same features with glacier metrics added as properties (server-side).
    """
    return ee.FeatureCollection(
        ee.FeatureCollection(lakes_fc).map(
            lambda f: _glacier_metrics_for_lake_polygon(ee.Feature(f), buffer_km)
        )
    )



# Step 1: DataFrame -> FC of points
points_fc = df_to_fc(df)

# Step 2: Map lake-detection wrapper to get polygons
def detect_area_glof(feature):
    point = feature.geometry()
    year  = ee.Number.parse(feature.get("Year_final")).toInt()
    lake  = detect_lake_pre_glof(point, year)
    # keep all properties and explicitly carry the area
    return ee.Feature(lake).copyProperties(feature).set(
        'Lake_area_calculated_ha', lake.get('Lake_area_calculated_ha')
    )

lakes_fc = points_fc.map(detect_area_glof)




import pandas as pd
df_pos_expansion10y=pd.read_csv("pos_expansion10y.csv")


df_pos_expansion10y.shape


df_pos_expansion10y5row=df_pos_expansion10y[:20]
df_pos_expansion10y5row



def fc_properties_to_df(fc, max_items=5000):
    fc_limited = ee.FeatureCollection(fc).limit(max_items)
    feats = fc_limited.getInfo()["features"]
    rows = [f["properties"] for f in feats]
    return pd.DataFrame(rows)







# DataFrame -> FC of points (has 'id' uniqued in df_to_fc)
import numpy as np
import pandas as pd

df_safe = df_pos_expansion10y5row.copy()
# Replace NaN and +/-inf with None (EE → null)
df_safe = df_safe.replace([np.nan, np.inf, -np.inf], None)

fc_points = df_to_fc(df_safe)

# Map your lake-detection wrapper to get polygons with area property
fc_polys = fc_points.map(detect_area_glof)

# buffer_km can be tuned; 50 km is your default
fc_polys_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)
df_glac_props = fc_properties_to_df(fc_polys_with_glac, max_items=len(df_pos_expansion10y5row))

# Keep only the columns you care about
df_glac_trial = df_glac_props[[
    "glacier_area_ha",             # glacier area in hectares
    "slope_glac_to_lake",          # slope from glacier to lake
    "glacier_contact",             # True/False if touching
    "glacier_touch_count",         # number of glaciers directly touching
    "nearest_glacier_dist_m",      # distance to nearest glacier (m)
    "glacier_elev_m",               # glacier elevation (m)
    "Latitude",
    "Longitude",
    "Year_final"

]]
df_glac_trial.head()





from tqdm.notebook import tqdm
import numpy as np
import pandas as pd

batch_size = 10
results = []

n_batches = (len(df_pos_expansion10y) + batch_size - 1) // batch_size

for i in tqdm(range(0, len(df_pos_expansion10y), batch_size), 
              total=n_batches, desc="Processing glacier batches"):
    df_batch = df_pos_expansion10y.iloc[i:i+batch_size].copy()
    df_batch = df_batch.replace([np.nan, np.inf, -np.inf], None)

    fc_points = df_to_fc(df_batch)
    fc_polys = fc_points.map(detect_area_glof)
    fc_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)

    # Remove geometries to lighten payload
    fc_with_glac = fc_with_glac.map(lambda f: f.setGeometry(None))

    data = fc_with_glac.getInfo()["features"]
    rows = [f["properties"] for f in data]
    results.append(pd.DataFrame(rows))

df_glac_updates = pd.concat(results, ignore_index=True)

df_glac_pos = pd.merge(
    df_pos_expansion10y,
    df_glac_updates[[
        "Latitude", "Longitude", "Year_final",
        "glacier_area_ha", "slope_glac_to_lake",
        "glacier_contact", "glacier_touch_count",
        "nearest_glacier_dist_m", "glacier_elev_m"
    ]],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)

print("Glacier features merged. Final dataframe shape:", df_glac_pos.shape)




df_glac_pos[:20]


df_glac_pos.to_csv("glac_pos_list.csv")











def detect_area_nonglof(feature):
    point = feature.geometry()
    year  = ee.Number.parse(feature.get("Year_final")).toInt()
    lake  = detect_lake_from_point(point, year)

    # Return the lake feature, with all original properties copied,
    # and explicitly keep the calculated area
    return ee.Feature(lake).copyProperties(feature).set(
        'Lake_area_calculated_ha', lake.get('Lake_area_calculated_ha')
    )



df_neg_expansion10y5row = pd.read_csv("neg_expansion10y.csv", nrows=5)
df_neg_expansion10y5row = df_neg_expansion10y5row.loc[:, ~df_neg_expansion10y5row.columns.str.contains("^Unnamed")]
df_neg_expansion10y5row


import numpy as np
import pandas as pd

df_safe = df_neg_expansion10y5row.copy()
# Replace NaN and +/-inf with None (EE → null)
df_safe = df_safe.replace([np.nan, np.inf, -np.inf], None)

fc_points = df_to_fc(df_safe)

# Map your lake-detection wrapper to get polygons with area property
fc_polys = fc_points.map(detect_area_nonglof)

# buffer_km can be tuned; 5 km here for faster run
fc_polys_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)

df_glac_props = fc_properties_to_df(fc_polys_with_glac, 
                                    max_items=len(df_neg_expansion10y5row))

# Keep only the columns you care about
df_glac_trial_neg = df_glac_props[[
    "glacier_area_ha",             # glacier area in hectares
    "slope_glac_to_lake",          # slope from glacier to lake
    "glacier_contact",             # True/False if touching
    "glacier_touch_count",         # number of glaciers directly touching
    "nearest_glacier_dist_m",      # distance to nearest glacier (m)
    "glacier_elev_m",              # glacier elevation (m)
    "Latitude",
    "Longitude",
    "Year_final",
    "Lake_area_calculated_ha",
    "Lake_type_simplified",
    "Lake_area_ha",
    "expansion_ha_peryr"
]]

# Show first rows
df_glac_trial_neg.head()






df_neg_expansion10y = pd.read_csv("neg_expansion10y.csv")
df_neg_expansion10y = df_neg_expansion10y.loc[:, ~df_neg_expansion10y.columns.str.contains("^Unnamed")]
df_neg_expansion10y[100:120]


from tqdm.notebook import tqdm
import numpy as np
import pandas as pd

batch_size = 15
results = []

n_batches = (len(df_neg_expansion10y) + batch_size - 1) // batch_size

for i in tqdm(range(0, len(df_neg_expansion10y), batch_size), 
              total=n_batches, desc="Processing negative glacier batches"):
    # --- slice + clean batch ---
    df_batch = df_neg_expansion10y.iloc[i:i+batch_size].copy()
    df_batch = df_batch.replace([np.nan, np.inf, -np.inf], None)

    # --- convert to FeatureCollection ---
    fc_points = df_to_fc(df_batch)
    fc_polys = fc_points.map(detect_area_nonglof)   # 👈 nonglof here
    fc_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)

    # remove geometries to lighten payload
    fc_with_glac = fc_with_glac.map(lambda f: f.setGeometry(None))

    # --- pull properties back client-side ---
    data = fc_with_glac.getInfo()["features"]
    rows = [f["properties"] for f in data]
    results.append(pd.DataFrame(rows))

# --- combine all batch outputs ---
df_glac_updates = pd.concat(results, ignore_index=True)

# --- merge back into master df ---
df_glac_neg = pd.merge(
    df_neg_expansion10y,
    df_glac_updates[[
        "Latitude", "Longitude", "Year_final",
        "glacier_area_ha", "slope_glac_to_lake",
        "glacier_contact", "glacier_touch_count",
        "nearest_glacier_dist_m", "glacier_elev_m"
    ]],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)

print("Negative glacier features merged. Final dataframe shape:", df_glac_neg.shape)



df_glac_neg.head()


df_glac_neg.to_csv("glac_neg_list.csv")


import ee

def _glacier_metrics_for_lake_polygon(lake, buffer_km=50):
    """
    Annotate ONE lake polygon with glacier metrics (final practical version).

    Rules:
      - If touching glaciers exist: contact=True, sum area, count them, slope=0.
      - Else: among non-touching glaciers with centroid elev > lake elev:
          pick nearest (poly-to-poly distance), slope = Δelev / distance.
      - If no valid glacier -> return null-like fields.
    """
    glaciers = ee.FeatureCollection("GLIMS/current")
    dem      = ee.Image("USGS/SRTMGL1_003")  # 30 m DEM

    lake_geom = ee.Feature(lake).geometry().buffer(1)

    # Lake centroid elevation
    lake_centroid = lake_geom.centroid()
    lake_elev = ee.Number(dem.sample(lake_centroid, 30).first().get("elevation"))

    # Candidate glaciers within buffer
    search = lake_geom.buffer(ee.Number(buffer_km).multiply(1000))
    nearby = glaciers.filterBounds(search)

    # Null-like return
    def _no_hits(f):
        return ee.Feature(f).set({
            "glacier_contact": False,
            "glacier_touch_count": 0,
            "nearest_glacier_dist_m": None,
            "lake_elev_m": lake_elev,
            "glacier_elev_m": None,
            "slope_glac_to_lake": None,
            "glacier_area_ha": None,
            "glacier_ids": ee.List([]),
            "buffer_km_used": buffer_km,
            "slope_method": "centroid_elev+poly_dist"
        })

    def _compute(f):
        # Add centroid metrics
        def add_centroid_metrics(g):
            g  = ee.Feature(g)
            gg = g.geometry()
            glac_centroid = gg.centroid()
            glac_elev = ee.Number(dem.sample(glac_centroid, 30).first().get("elevation"))
            dist_poly = gg.distance(lake_geom, 1)
            area_ha   = ee.Number(gg.area(1)).divide(1e4)
            return g.set({
                "glac_elev_cent": glac_elev,
                "dist_poly":      dist_poly,
                "glacier_area_ha": area_ha,
                "glac_id":        g.get("glac_id"),
                "GLIMS_ID":       g.get("GLIMS_ID")
            })

        with_metrics = nearby.map(add_centroid_metrics)

        # Touching vs non-touching
        touching   = with_metrics.filter(ee.Filter.eq("dist_poly", 0))
        non_touch  = with_metrics.filter(ee.Filter.gt("dist_poly", 0))

        def _touching_case():
            touch_ids   = touching.aggregate_array("glac_id")
            touch_area  = ee.Number(touching.aggregate_sum("glacier_area_ha"))
            touch_count = touching.size()
            max_elev    = ee.Number(touching.aggregate_max("glac_elev_cent"))
            return ee.Feature(f).set({
                "glacier_contact":        True,
                "glacier_touch_count":    touch_count,
                "nearest_glacier_dist_m": 0,
                "lake_elev_m":            lake_elev,
                "glacier_elev_m":         max_elev,
                "slope_glac_to_lake":     0,
                "glacier_area_ha":        touch_area,
                "glacier_ids":            touch_ids,
                "buffer_km_used":         buffer_km,
                "slope_method":           "touching_centroid"
            })

        def _non_touching_case():
            # Only glaciers strictly higher than lake
            higher = non_touch.filter(ee.Filter.gt("glac_elev_cent", lake_elev))

            def _no_valid():
                return _no_hits(f)

            def _emit_final():
                best = ee.Feature(ee.FeatureCollection(higher).sort("dist_poly").first())
                min_dist   = ee.Number(best.get("dist_poly"))
                glac_elev  = ee.Number(best.get("glac_elev_cent"))
                slope      = glac_elev.subtract(lake_elev).divide(min_dist)
                chosen_id  = ee.Algorithms.If(best.get("glac_id"), best.get("glac_id"), best.get("GLIMS_ID"))
                return ee.Feature(f).set({
                    "glacier_contact":        False,
                    "glacier_touch_count":    0,
                    "nearest_glacier_dist_m": min_dist,
                    "lake_elev_m":            lake_elev,
                    "glacier_elev_m":         glac_elev,
                    "slope_glac_to_lake":     slope,
                    "glacier_area_ha":        best.get("glacier_area_ha"),
                    "glacier_ids":            ee.List([chosen_id]),
                    "buffer_km_used":         buffer_km,
                    "slope_method":           "centroid_elev+poly_dist"
                })

            return ee.Feature(ee.Algorithms.If(higher.size().eq(0), _no_valid(), _emit_final()))

        return ee.Feature(
            ee.Algorithms.If(touching.size().gt(0), _touching_case(), _non_touching_case())
        )

    return ee.Feature(
        ee.Algorithms.If(nearby.size().eq(0), _no_hits(lake), _compute(lake))
    )

# --- 2) Public: map over a FeatureCollection whose geometry is a LAKE POLYGON (or point) ---
def annotate_glacier_metrics_from_polygons(lakes_fc, buffer_km=10):
    """
    Input:  FeatureCollection where geometry is the LAKE polygon.
            (If some are points, they are buffered 1 m internally so distance works.)
    Output: Same features with glacier metrics added as properties (server-side).
    """
    return ee.FeatureCollection(
        ee.FeatureCollection(lakes_fc).map(
            lambda f: _glacier_metrics_for_lake_polygon(ee.Feature(f), buffer_km)
        )
    )

# DataFrame -> FC of points (has 'id' uniqued in df_to_fc)
import numpy as np
import pandas as pd

df_safe = df_pos_expansion10y5row.copy()
# Replace NaN and +/-inf with None (EE → null)
df_safe = df_safe.replace([np.nan, np.inf, -np.inf], None)

fc_points = df_to_fc(df_safe)

# Map your lake-detection wrapper to get polygons with area property
fc_polys = fc_points.map(detect_area_glof)

# buffer_km can be tuned; 50 km is your default
fc_polys_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)
df_glac_props = fc_properties_to_df(fc_polys_with_glac, max_items=len(df_pos_expansion10y5row))

# Keep only the columns you care about
df_glac_trial = df_glac_props[[
    "glacier_area_ha",             # glacier area in hectares
    "slope_glac_to_lake",          # slope from glacier to lake
    "glacier_contact",             # True/False if touching
    "glacier_touch_count",         # number of glaciers directly touching
    "nearest_glacier_dist_m",      # distance to nearest glacier (m)
    "glacier_elev_m",               # glacier elevation (m)
    "Latitude",
    "Longitude",
    "Year_final"

]]
df_glac_trial.head()



from tqdm.notebook import tqdm
import numpy as np
import pandas as pd

batch_size = 5
results = []

n_batches = (len(df_pos_expansion10y) + batch_size - 1) // batch_size

for i in tqdm(range(0, len(df_pos_expansion10y), batch_size), 
              total=n_batches, desc="Processing glacier batches"):
    df_batch = df_pos_expansion10y.iloc[i:i+batch_size].copy()
    df_batch = df_batch.replace([np.nan, np.inf, -np.inf], None)

    fc_points = df_to_fc(df_batch)
    fc_polys = fc_points.map(detect_area_glof)
    fc_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)

    # Remove geometries to lighten payload
    fc_with_glac = fc_with_glac.map(lambda f: f.setGeometry(None))

    data = fc_with_glac.getInfo()["features"]
    rows = [f["properties"] for f in data]
    results.append(pd.DataFrame(rows))

df_glac_updates = pd.concat(results, ignore_index=True)

df_glac_pos = pd.merge(
    df_pos_expansion10y,
    df_glac_updates[[
        "Latitude", "Longitude", "Year_final",
        "glacier_area_ha", "slope_glac_to_lake",
        "glacier_contact", "glacier_touch_count",
        "nearest_glacier_dist_m", "glacier_elev_m"
    ]],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)

print("✅ Glacier features merged. Final dataframe shape:", df_glac_pos.shape)


df_glac_pos.head()


df_glac_pos.to_csv("glac_pos_list_correct_slope.csv")


from tqdm.notebook import tqdm
import numpy as np
import pandas as pd

batch_size = 10
results = []

n_batches = (len(df_neg_expansion10y) + batch_size - 1) // batch_size

for i in tqdm(range(0, len(df_neg_expansion10y), batch_size), 
              total=n_batches, desc="Processing negative glacier batches"):
    # --- slice + clean batch ---
    df_batch = df_neg_expansion10y.iloc[i:i+batch_size].copy()
    df_batch = df_batch.replace([np.nan, np.inf, -np.inf], None)

    # --- convert to FeatureCollection ---
    fc_points = df_to_fc(df_batch)
    fc_polys = fc_points.map(detect_area_nonglof)   # 👈 nonglof here
    fc_with_glac = annotate_glacier_metrics_from_polygons(fc_polys, buffer_km=5)

    # remove geometries to lighten payload
    fc_with_glac = fc_with_glac.map(lambda f: f.setGeometry(None))

    # --- pull properties back client-side ---
    data = fc_with_glac.getInfo()["features"]
    rows = [f["properties"] for f in data]
    results.append(pd.DataFrame(rows))

# --- combine all batch outputs ---
df_glac_updates = pd.concat(results, ignore_index=True)

# --- merge back into master df ---
df_glac_neg = pd.merge(
    df_neg_expansion10y,
    df_glac_updates[[
        "Latitude", "Longitude", "Year_final",
        "glacier_area_ha", "slope_glac_to_lake",
        "glacier_contact", "glacier_touch_count",
        "nearest_glacier_dist_m", "glacier_elev_m"
    ]],
    on=["Latitude", "Longitude", "Year_final"],
    how="left"
)

print(" Negative glacier features merged. Final dataframe shape:", df_glac_neg.shape)


df_glac_neg.head()


df_glac_neg.to_csv("glac_neg_list_correct_slope.csv")



